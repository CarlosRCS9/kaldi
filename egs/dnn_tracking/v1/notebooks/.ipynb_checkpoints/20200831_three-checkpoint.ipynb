{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "spath = '../scripts'\n",
    "sys.path.insert(0, spath)\n",
    "\n",
    "from models import *\n",
    "from notebook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/export/b03/carlosc/data/2020/augmented'\n",
    "embedding_dim = 128\n",
    "\n",
    "dev_rttm     = data_folder + '/callhome/callhome1/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/ref.rttm'\n",
    "dev_segments = data_folder + '/callhome/callhome1/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/segments'\n",
    "dev_ivectors = data_folder + '/callhome/callhome1/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/exp/make_ivectors/ivector.txt'\n",
    "dev_xvectors = data_folder + '/callhome/callhome1/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/exp/make_xvectors/xvector.txt'\n",
    "dev_files_segments = get_rttm_segments_features(dev_rttm, dev_segments, dev_ivectors, dev_xvectors)\n",
    "\n",
    "eval_rttm     = data_folder + '/callhome/callhome2/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/ref.rttm'\n",
    "eval_segments = data_folder + '/callhome/callhome2/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/segments'\n",
    "eval_ivectors = data_folder + '/callhome/callhome2/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/exp/make_ivectors/ivector.txt'\n",
    "eval_xvectors = data_folder + '/callhome/callhome2/augmented_0/1.0_0.5_0.5/' + str(embedding_dim) + '/exp/make_xvectors/xvector.txt'\n",
    "eval_files_segments = get_rttm_segments_features(eval_rttm, eval_segments, eval_ivectors, eval_xvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 39533 of 70546: 0.5603861310350693\n",
      "Kept 38688 of 66651: 0.5804564072557051\n"
     ]
    }
   ],
   "source": [
    "dev_files_segments_lim = limit_segments_speakers_length(dev_files_segments, 1, log = True)\n",
    "eval_files_segments_lim = limit_segments_speakers_length(eval_files_segments, 1, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def invert_speaker_weights(speakers_weights):\n",
    "    weight_sum = sum(speakers_weights.values())\n",
    "    weight_count = len(speakers_weights.values())\n",
    "    speakers_weights_inverse = {}\n",
    "    for speaker_name, weight in speakers_weights.items():\n",
    "        speakers_weights_inverse[speaker_name] = (weight_sum - speakers_weights[speaker_name]) / ((weight_count - 1) * weight_sum)\n",
    "    return speakers_weights_inverse\n",
    "\n",
    "class Files_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 files_segments,\n",
    "                 files_speakers = {},\n",
    "                 models_generation_lengths = [3],\n",
    "                 models_container_length = 2,\n",
    "                 include_zeros = True,\n",
    "                 include_overlaps = False,\n",
    "                 feature = 'ivectors',\n",
    "                 zeros_multiplier = 1.0):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.files_segments = files_segments\n",
    "        self.files_speakers = {}\n",
    "        for file_id in self.files_segments:\n",
    "            self.files_speakers[file_id] = files_speakers[file_id] \\\n",
    "            if file_id in files_speakers \\\n",
    "            else get_first_speakers(self.files_segments[file_id])\n",
    "        self.models_generation_lengths = models_generation_lengths\n",
    "        self.models_container_length = models_container_length\n",
    "        self.include_zeros = include_zeros\n",
    "        self.include_overlaps = include_overlaps\n",
    "        self.feature = feature\n",
    "        self.zeros_multiplier = zeros_multiplier\n",
    "        self.speakers_segments_indexes = {}\n",
    "        self.speakers_models = {}\n",
    "        self.speakers_models_combinations_length = {}\n",
    "        self.complete_turns = {}\n",
    "        self.speakers_permutations = {}\n",
    "        self.speakers_weights = {}\n",
    "        self.lookup = []\n",
    "        self.length = 0\n",
    "        for file_id, segments in self.files_segments.items():\n",
    "            self.speakers_segments_indexes[file_id] = get_speakers_segments_indexes(enumerate(segments))\n",
    "            self.speakers_models[file_id] = get_speakers_models(segments,\n",
    "                                                                self.speakers_segments_indexes[file_id],\n",
    "                                                                self.models_generation_lengths,\n",
    "                                                                self.files_speakers[file_id])\n",
    "            self.speakers_models_combinations_length[file_id] = len(self.models_generation_lengths) ** len(self.speakers_models[file_id].keys())            \n",
    "            self.complete_turns[file_id] = 4\n",
    "            \n",
    "            length = self.complete_turns[file_id] * self.speakers_models_combinations_length[file_id] * len(segments)\n",
    "            self.speakers_permutations[file_id] = Permutations(self.speakers_models[file_id].keys(),\n",
    "                                                               length,\n",
    "                                                               self.models_container_length,\n",
    "                                                               self.include_zeros)\n",
    "            speakers_weights = self.speakers_permutations[file_id].get_speakers_names_counts()\n",
    "            if self.include_zeros and '0' in speakers_weights:\n",
    "                speakers_weights['0'] *= (1 / self.zeros_multiplier)\n",
    "            self.speakers_weights[file_id] = invert_speaker_weights(speakers_weights)\n",
    "\n",
    "            self.lookup.append({ 'file_id': file_id, 'onset': self.length, 'end': self.length + length - 1 })\n",
    "            self.length += length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        lookup = [value for value in self.lookup if value['onset'] <= key and key <= value['end']][0]\n",
    "        file_id = lookup['file_id']\n",
    "        index = key - lookup['onset']\n",
    "        segments = self.files_segments[file_id]\n",
    "        permutation = self.speakers_permutations[file_id][index]\n",
    "        _, turn_index = divmod(index, self.speakers_models_combinations_length[file_id] * len(segments))\n",
    "        \n",
    "        model_index, segment_index = divmod(turn_index, len(segments))\n",
    "        \n",
    "        models = {}\n",
    "        models_speakers_names = list(self.speakers_models[file_id].keys())\n",
    "        remainder = model_index\n",
    "        for speaker_name_index, speaker_name in enumerate(models_speakers_names):\n",
    "            models_lengths = [len(self.speakers_models[file_id][speaker_name].keys()) for speaker_name in models_speakers_names[speaker_name_index + 1:]]\n",
    "            if index != len(models_speakers_names) - 1:\n",
    "                model_index, remainder = divmod(remainder, int(np.prod(models_lengths)))\n",
    "            else:\n",
    "                model_index = remainder\n",
    "            models[speaker_name] = self.speakers_models[file_id][speaker_name][list(self.speakers_models[file_id][speaker_name].keys())[model_index]]\n",
    "\n",
    "        models_container = [models[speaker_name] if speaker_name != '0' else { 'ivectors': [Ivector(np.random.uniform(-0.1, 0.1, self.embedding_dim).astype(np.float32))], 'xvectors': [Xvector(np.random.uniform(-0.1, 0.1, self.embedding_dim).astype(np.float32))]} for speaker_name in permutation]\n",
    "        \n",
    "        segment = segments[segment_index]\n",
    "        segment_speakers_names = [speaker.get_name() for speaker in segment.get_speakers()]\n",
    "        \n",
    "        x = [embeddings[self.feature][0].get_value() for embeddings in models_container + [{ 'ivectors': segment.get_ivectors(), 'xvectors': segment.get_xvectors() }]]\n",
    "        y = np.asarray([speaker_name in segment_speakers_names for speaker_name in permutation], dtype = np.float32)\n",
    "        w =  np.asarray([self.speakers_weights[file_id][speaker_name] for speaker_name in permutation], dtype = np.float32)\n",
    "        return x, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "'''class Model(nn.Module):\n",
    "    def __init__(self, b, M):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d((M + 1), M ** 3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(M ** 3, M ** 2, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(M ** 2, M, 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lstm1 = nn.Sequential(\n",
    "            nn.LSTM(b - 6, 32, bidirectional = True),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(M * 2 * 32, M * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M * 16, M * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M * 8, M),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.stack(x, 1)\n",
    "        x = self.cnn1(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x'''\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, b, M):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d((M + 1), M ** 3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(M ** 3, M ** 2, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(M ** 2, M, 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lstm1 = nn.Sequential(\n",
    "            nn.LSTM(b - 6, 8, bidirectional = True),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(M * 2 * 8, M * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M * 4, M * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M * 2, M),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.stack(x, 1)\n",
    "        x = self.cnn1(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Train_graph:\n",
    "    def __init__(self, training_loss, validation_loss = None):\n",
    "        self.step = 0\n",
    "        self.training_losses_x = [self.step - 1, self.step]\n",
    "        self.training_losses_y = [training_loss, training_loss]\n",
    "        self.validation_losses_x = None if validation_loss is None else [self.step - 1, self.step]\n",
    "        self.validation_losses_y = None if validation_loss is None else [validation_loss, validation_loss]\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        self.ax = self.fig.add_subplot()\n",
    "        \n",
    "        self.training_line, = self.ax.plot(self.training_losses_x, self.training_losses_y, '--', label = 'Training')\n",
    "        self.validation_line = None if validation_loss is None else self.ax.plot(self.validation_losses_x, self.validation_losses_y, label = 'Validation')[0]\n",
    "            \n",
    "        self.ax.set_ylim(0, training_loss if validation_loss is None else (training_loss if training_loss > validation_loss else validation_loss))\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        self.ax.legend()\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "    def draw(self, training_loss, validation_loss = None):\n",
    "        self.step += 1\n",
    "        self.training_losses_x.append(self.step)\n",
    "        self.training_losses_y.append(training_loss)\n",
    "        self.training_line.set_xdata(self.training_losses_x)\n",
    "        self.training_line.set_ydata(self.training_losses_y)\n",
    "        \n",
    "        if validation_loss is not None:\n",
    "            if self.validation_line is None:\n",
    "                self.validation_losses_x = [self.step - 1]\n",
    "                self.validation_losses_y = [validation_loss]\n",
    "                self.validation_line = self.ax.plot(self.validation_losses_x, self.validation_losses_y, label = 'Validation')[0]\n",
    "                self.ax.legend()\n",
    "            self.validation_losses_x.append(self.step)\n",
    "            self.validation_losses_y.append(validation_loss)\n",
    "            self.validation_line.set_xdata(self.validation_losses_x)\n",
    "            self.validation_line.set_ydata(self.validation_losses_y)\n",
    "        self.ax.set_xlim(0, self.step + 1)\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-8-bb57bff39082>\", line 94, in __getitem__\n    x = [embeddings[self.feature][0].get_value() for embeddings in models_container + [{ 'ivectors': segment.get_ivectors(), 'xvectors': segment.get_xvectors() }]]\n  File \"<ipython-input-8-bb57bff39082>\", line 94, in <listcomp>\n    x = [embeddings[self.feature][0].get_value() for embeddings in models_container + [{ 'ivectors': segment.get_ivectors(), 'xvectors': segment.get_xvectors() }]]\nKeyError: 'xvectors'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-77ade6356e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdev_train_dataloader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-8-bb57bff39082>\", line 94, in __getitem__\n    x = [embeddings[self.feature][0].get_value() for embeddings in models_container + [{ 'ivectors': segment.get_ivectors(), 'xvectors': segment.get_xvectors() }]]\n  File \"<ipython-input-8-bb57bff39082>\", line 94, in <listcomp>\n    x = [embeddings[self.feature][0].get_value() for embeddings in models_container + [{ 'ivectors': segment.get_ivectors(), 'xvectors': segment.get_xvectors() }]]\nKeyError: 'xvectors'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "seed = 19970917 #25005233 19970917\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "models_container_length = 6\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "dev_files_ids = list(dev_files_segments_lim.keys())\n",
    "dev_validation_files_ids = random.sample(dev_files_ids, int(len(dev_files_ids) * 0.3))\n",
    "dev_train_files_ids = [file_id for file_id in dev_files_ids if file_id not in dev_validation_files_ids]\n",
    "\n",
    "dev_train_files_segments = {}\n",
    "for file_id in dev_train_files_ids:\n",
    "    dev_train_files_segments[file_id] = dev_files_segments_lim[file_id]\n",
    "dev_validation_files_segments = {}\n",
    "for file_id in dev_validation_files_ids:\n",
    "    dev_validation_files_segments[file_id] = dev_files_segments_lim[file_id]\n",
    "\n",
    "dev_train_dataset = Files_dataset(embedding_dim,\n",
    "                                  dev_train_files_segments,\n",
    "                                  models_generation_lengths = [5],\n",
    "                                  models_container_length = models_container_length,\n",
    "                                  feature = 'xvectors')\n",
    "dev_validation_dataset = Files_dataset(embedding_dim,\n",
    "                                       dev_validation_files_segments,\n",
    "                                       models_generation_lengths = [5],\n",
    "                                       models_container_length = models_container_length,\n",
    "                                       feature = 'xvectors')\n",
    "\n",
    "dev_train_dataloader = DataLoader(dev_train_dataset, batch_size = 32, num_workers = 8)\n",
    "dev_validation_dataloader = DataLoader(dev_validation_dataset, batch_size = 32, num_workers = 8)\n",
    "\n",
    "net = Model(embedding_dim, models_container_length).to(device, non_blocking = True)\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for dataloader in [dev_train_dataloader]:\n",
    "        for x, y, w in dataloader:\n",
    "            x = [tensor.to(device, non_blocking = True).float() for tensor in x]\n",
    "            y = y.to(device, non_blocking = True).float()\n",
    "            w = w.to(device, non_blocking = True).float()\n",
    "            criterion = nn.BCELoss(w)\n",
    "            y_ = net(x)\n",
    "            loss = criterion(y_, y)\n",
    "            losses.append(loss.data)\n",
    "train_loss = (np.sum(losses) / len(losses)).cpu()\n",
    "\n",
    "validation_losses = []\n",
    "with torch.no_grad():\n",
    "    for dataloader in [dev_validation_dataloader]:\n",
    "        for x, y, w in dataloader:\n",
    "            x = [tensor.to(device, non_blocking = True).float() for tensor in x]\n",
    "            y = y.to(device, non_blocking = True).float()\n",
    "            w = w.to(device, non_blocking = True).float()\n",
    "            criterion = nn.BCELoss(w)\n",
    "            y_ = net(x)\n",
    "            loss = criterion(y_, y)\n",
    "            validation_losses.append(loss.data)\n",
    "validation_loss = (np.sum(validation_losses) / len(validation_losses)).cpu()\n",
    "\n",
    "\n",
    "train_graph = Train_graph(train_loss, validation_loss)\n",
    "\n",
    "for train_dataloader, validation_dataloader in [(dev_train_dataloader, dev_validation_dataloader)]:\n",
    "    for epoch in range(15):\n",
    "        losses = []\n",
    "        for x, y, w in train_dataloader:\n",
    "            x = [tensor.to(device, non_blocking = True).float() for tensor in x]\n",
    "            y = y.to(device, non_blocking = True).float()\n",
    "            w = w.to(device, non_blocking = True).float()\n",
    "\n",
    "            criterion = nn.BCELoss(w)\n",
    "            net.zero_grad()\n",
    "            y_ = net(x)\n",
    "            loss = criterion(y_, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.data)\n",
    "        train_loss = np.sum(losses) / len(losses)\n",
    "        \n",
    "        validation_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, w in validation_dataloader:\n",
    "                x = [tensor.to(device, non_blocking = True).float() for tensor in x]\n",
    "                y = y.to(device, non_blocking = True).float()\n",
    "                w = w.to(device, non_blocking = True).float()\n",
    "                criterion = nn.BCELoss(w)\n",
    "                y_ = net(x)\n",
    "                loss = criterion(y_, y)\n",
    "                \n",
    "                validation_losses.append(loss)\n",
    "        validation_loss = np.sum(validation_losses) / len(validation_losses)\n",
    "        \n",
    "        train_graph.draw(train_loss, validation_loss)\n",
    "        print(train_loss, validation_loss, end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import kaldi_utils\n",
    "\n",
    "class File_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 segments,\n",
    "                 speakers = [],\n",
    "                 models_generation_length = 5,\n",
    "                 models_container_length = 2,\n",
    "                 feature = 'ivectors'):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.segments = segments\n",
    "        self.speakers = speakers[:models_container_length]\n",
    "        if len(self.speakers) < models_container_length:\n",
    "            speakers = [speaker for speaker in get_first_speakers(segments) if speaker not in self.speakers]\n",
    "            self.speakers += speakers[:models_container_length - len(self.speakers)]\n",
    "            self.speakers += ['0' for i in range(models_container_length - len(self.speakers))]                \n",
    "        self.models_generation_length = models_generation_length\n",
    "        self.models_container_length = models_container_length\n",
    "        self.feature = feature\n",
    "        self.length = len(segments)\n",
    "        \n",
    "        self.speakers_segments_indexes = get_speakers_segments_indexes(enumerate(segments))\n",
    "        self.speakers_models = get_speakers_models(segments,\n",
    "                                                   self.speakers_segments_indexes,\n",
    "                                                   [self.models_generation_length],\n",
    "                                                   self.speakers)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        segment = self.segments[key]\n",
    "        #print(self.speakers, segment.get_turn_onset(), [speaker.get_name() for speaker in segment.get_speakers()], end = '')\n",
    "        models_container = [self.speakers_models[speaker][self.models_generation_length] if speaker != '0' else { 'ivectors': [Ivector(np.random.uniform(-0.1, 0.1, self.embedding_dim).astype(np.float32))] } for speaker in self.speakers]\n",
    "        x = [embeddings[self.feature][0].get_value() for embeddings in models_container + [{ 'ivectors': segment.get_ivectors() }]]\n",
    "        return x, key\n",
    "    \n",
    "def net_scoring(x, net, device):\n",
    "    with torch.no_grad():\n",
    "        x = [tensor.to(device) for tensor in x]\n",
    "        y_ = net(x)\n",
    "        return y_.cpu().numpy()[0]\n",
    "    \n",
    "def get_custom_rttm(segment, speaker_name):\n",
    "    return segment.get_type() + ' ' + \\\n",
    "        segment.get_file_id() + ' ' + \\\n",
    "        segment.get_speakers()[0].get_channel_id() + ' ' + \\\n",
    "        str(round(segment.get_turn_onset(), 3)) + ' ' + \\\n",
    "        str(round(segment.get_turn_duration(), 3)) + ' ' + \\\n",
    "        segment.get_orthography_field() + ' ' + \\\n",
    "        segment.get_speakers()[0].get_type() + ' ' + \\\n",
    "        speaker_name + ' ' + \\\n",
    "        segment.get_confidence_score() + ' ' + \\\n",
    "        segment.get_signal_lookahead_time() + '\\n'\n",
    "\n",
    "eval_files_speakers = get_first_speakers(eval_files_segments, models_container_length)\n",
    "\n",
    "groundtruth_rttm = ''\n",
    "result_rttm = ''\n",
    "post_rttm = ''\n",
    "for file_id, segments in eval_files_segments_lim.items():\n",
    "    groundtruth_segments = []\n",
    "    result_segments = []\n",
    "    file_dataset = File_dataset(embedding_dim, segments, models_container_length = models_container_length)\n",
    "    file_dataloader = DataLoader(file_dataset)\n",
    "    for x, segment_index in file_dataloader:\n",
    "        segment = segments[segment_index]\n",
    "        if len(groundtruth_segments) > 0 \\\n",
    "        and groundtruth_segments[-1].has_overlap(segment) \\\n",
    "        and groundtruth_segments[-1].has_same_speakers(segment):\n",
    "            groundtruth_segments[-1].set_turn_end(segment.get_turn_end())\n",
    "        else:\n",
    "            groundtruth_segments.append(segment)\n",
    "\n",
    "        y_ = net_scoring(x, net, device)\n",
    "        index = np.argmax(y_)\n",
    "        result_segment = Segment(segment)\n",
    "        result_speaker = result_segment.get_speakers()[0]\n",
    "        result_speaker.set_name(str(index))\n",
    "        result_segment.set_speakers([result_speaker])\n",
    "        if len(result_segments) > 0 \\\n",
    "        and result_segments[-1].has_overlap(result_segment) \\\n",
    "        and result_segments[-1].has_same_speakers(result_segment):\n",
    "            result_segments[-1].set_turn_end(result_segment.get_turn_end())\n",
    "        else:\n",
    "            result_segments.append(result_segment)\n",
    "\n",
    "    post_segments = [Segment(result_segments[0])]\n",
    "    for index, t_segment in enumerate(result_segments):\n",
    "        if index > 1:\n",
    "            t_1_segment = result_segments[index - 1]\n",
    "            t_2_segment = post_segments[-1]\n",
    "            if t_segment.has_overlap(t_2_segment) and t_segment.has_overlap(t_1_segment) \\\n",
    "            and t_segment.has_same_speakers(t_2_segment):\n",
    "                post_segments[-1].set_turn_end(t_1_segment.get_turn_end())\n",
    "            else:\n",
    "                post_segments.append(Segment(t_1_segment))\n",
    "    if post_segments[-1].has_overlap(result_segments[-1]) \\\n",
    "    and post_segments[-1].has_same_speakers(result_segments[-1]):\n",
    "        post_segments[-1].set_turn_end(result_segments[-1].get_turn_end())\n",
    "    else:\n",
    "        post_segments.append(Segment(result_segments[-1]))            \n",
    "        \n",
    "    groundtruth_rttm += ''.join([segment.get_rttm() for segment in groundtruth_segments])\n",
    "    result_rttm += ''.join([segment.get_rttm() for segment in result_segments])\n",
    "    post_rttm += ''.join([segment.get_rttm() for segment in post_segments])\n",
    "    \n",
    "        \n",
    "tmp_folder = 'batch/20200831'\n",
    "!mkdir -p $tmp_folder\n",
    "\n",
    "file = open(tmp_folder + '/groundtruth.rttm', 'w')\n",
    "file.write(groundtruth_rttm)\n",
    "file.close()\n",
    "        \n",
    "file = open(tmp_folder + '/result.rttm', 'w')\n",
    "file.write(result_rttm)\n",
    "file.close()\n",
    "\n",
    "file = open(tmp_folder + '/post.rttm', 'w')\n",
    "file.write(post_rttm)\n",
    "file.close()\n",
    "    \n",
    "kaldi_utils.md_eval(tmp_folder + '/groundtruth.rttm', tmp_folder + '/result.rttm', tmp_folder)\n",
    "!cat $tmp_folder/der.log\n",
    "\n",
    "kaldi_utils.md_eval(tmp_folder + '/groundtruth.rttm', tmp_folder + '/post.rttm', tmp_folder)\n",
    "!cat $tmp_folder/der.log\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
