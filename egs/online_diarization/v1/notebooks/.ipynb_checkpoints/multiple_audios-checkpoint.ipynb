{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: 249/249\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def is_valid_segment(segment):\n",
    "    return len(segment['speakers']) == 1 \\\n",
    "            and len(segment['ivectors']) == 1 \\\n",
    "            and len(segment['xvectors']) == 1 \\\n",
    "            and segment['speakers'][0]['speaker_id'] in ['A', 'B']\n",
    "\n",
    "directory = '../exp/json'\n",
    "filenames = [filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n",
    "\n",
    "recordings_segments = {}\n",
    "recordings_length = len(filenames)\n",
    "recordings_count = 0\n",
    "for filename in filenames:\n",
    "    recording_id = filename.split('.')[0]\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    file = open(filepath, 'r')\n",
    "    recordings_segments[recording_id] = [json.loads(line) for line in file.readlines()]\n",
    "    file.close()\n",
    "    recordings_segments[recording_id] = list(filter(is_valid_segment, recordings_segments[recording_id]))\n",
    "    recordings_count += 1\n",
    "    print('Loading data: ' + str(recordings_count) + '/' + str(recordings_length), end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def speakers_get_indexes(accumulator, speaker_tuple):\n",
    "    speaker_id, index = speaker_tuple\n",
    "    if speaker_id in accumulator:\n",
    "        accumulator[speaker_id].append(index)\n",
    "    else:\n",
    "        accumulator[speaker_id] = [index]\n",
    "    return accumulator\n",
    "\n",
    "recordings_segments_cut = {}\n",
    "for recording_id in recordings_segments:\n",
    "    recording_segments = recordings_segments[recording_id]\n",
    "    speakers_indexes = [(segment['speakers'][0]['speaker_id'], index) for index, segment in enumerate(recording_segments)]\n",
    "    speakers_indexes = reduce(speakers_get_indexes, speakers_indexes, {})\n",
    "    speakers_lengths = [(speaker_id, len(speakers_indexes[speaker_id])) for speaker_id in speakers_indexes]\n",
    "    speakers_lengths.sort(key = lambda x: x[1])\n",
    "    speakers_lengths_min = speakers_lengths[0][1]\n",
    "    if len(speakers_lengths) > 1 and speakers_lengths_min > 20: # <-- IMPORTANT\n",
    "        recording_indexes = []\n",
    "        for speaker_id in speakers_indexes:\n",
    "            speakers_indexes[speaker_id] = speakers_indexes[speaker_id][:speakers_lengths_min]\n",
    "            recording_indexes += speakers_indexes[speaker_id]\n",
    "        recordings_segments_cut[recording_id] = [segment for index, segment in enumerate(recordings_segments[recording_id]) if index in recording_indexes]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "models_generation_length = 3 # <-- IMPORTANT\n",
    "models_container_length = 2  # <-- IMPORTANT\n",
    "permutations_include_zeros = False\n",
    "\n",
    "class Recordings_dataset(Dataset):\n",
    "    def __init__(self, recordings_segments, recordings_ids, mode = 'ivectors'):\n",
    "        self.recordings_ids = recordings_ids if isinstance(recordings_ids, list) else [recordings_ids]\n",
    "        self.recordings_segments = {}\n",
    "        for recording_id in self.recordings_ids:\n",
    "            self.recordings_segments[recording_id] = recordings_segments[recording_id]\n",
    "        self.mode = mode\n",
    "        self.models_generation_length = models_generation_length\n",
    "        self.models_container_length = models_container_length\n",
    "        self.permutations_include_zeros = permutations_include_zeros\n",
    "        self.recordings_data = {}\n",
    "        self.recordings_map = []\n",
    "        self.recordings_length = 0\n",
    "        for recording_id in self.recordings_ids:\n",
    "            self.recordings_data[recording_id] = {}\n",
    "            recording_segments = self.recordings_segments[recording_id]\n",
    "            recording_data = self.recordings_data[recording_id]\n",
    "            recording_data['speakers_indexes'] = [(segment['speakers'][0]['speaker_id'], index) for index, segment in enumerate(recording_segments)]\n",
    "            recording_data['speakers_indexes'] = reduce(speakers_get_indexes, recording_data['speakers_indexes'], {})\n",
    "            recording_data['speakers_models'] = {}\n",
    "            for speaker_id in recording_data['speakers_indexes']:\n",
    "                speaker_indexes = recording_data['speakers_indexes'][speaker_id]\n",
    "                speaker_vectors = [recording_segments[index][self.mode][0]['value'] for index in speaker_indexes[:self.models_generation_length]]\n",
    "                recording_data['speakers_models'][speaker_id] = [np.sum(speaker_vectors, 0) / len(speaker_vectors)]\n",
    "            if self.permutations_include_zeros:\n",
    "                recording_data['permutations'] = list(itertools.permutations(list(recording_data['speakers_models'].keys()) \\\n",
    "                + ['0' for i in range(self.models_container_length)], self.models_container_length))\n",
    "            else:\n",
    "                recording_data['permutations'] = list(itertools.permutations(list(recording_data['speakers_models'].keys()), self.models_container_length))\n",
    "            recording_data['permutations'] = list(set(recording_data['permutations']))\n",
    "            recording_data['permutations'].sort()\n",
    "            recording_data['permutations_map'] = []\n",
    "            recording_data['permutations_length'] = 0\n",
    "            for index, permutation in enumerate(recording_data['permutations']):\n",
    "                speakers_models_length = int(np.prod([len(recording_data['speakers_models'][speaker_id]) for speaker_id in permutation if speaker_id != '0']))\n",
    "                recording_data['permutations_map'].append((recording_data['permutations_length'], recording_data['permutations_length'] + speakers_models_length - 1, index))\n",
    "                recording_data['permutations_length'] += speakers_models_length\n",
    "            recording_data['length'] = len(recording_segments) * recording_data['permutations_length']\n",
    "            self.recordings_map.append((self.recordings_length, self.recordings_length + recording_data['length'] - 1, recording_id))\n",
    "            self.recordings_length += recording_data['length']\n",
    "    def __len__(self):\n",
    "        return self.recordings_length\n",
    "    def __getitem__(self, idx):\n",
    "        recording_tuple = list(filter(lambda recording_tuple: recording_tuple[0] <= idx and idx <= recording_tuple[1], self.recordings_map))[0]\n",
    "        recording_idx = idx - recording_tuple[0]\n",
    "        recording_id = recording_tuple[2]\n",
    "        recording_data = self.recordings_data[recording_id]\n",
    "        \n",
    "        segment_id, segment_idx = divmod(recording_idx, recording_data['permutations_length'])\n",
    "        segment = self.recordings_segments[recording_id][segment_id]\n",
    "        target_id = segment['speakers'][0]['speaker_id']\n",
    "        vector = segment[self.mode][0]['value']\n",
    "        \n",
    "        permutation_tuple = list(filter(lambda permutation_tuple: permutation_tuple[0] <= segment_idx and segment_idx <= permutation_tuple[1], recording_data['permutations_map']))[0]\n",
    "        permutation_id = permutation_tuple[2]\n",
    "        permutation = recording_data['permutations'][permutation_id]\n",
    "        \n",
    "        models_container = [recording_data['speakers_models'][speaker_id][0] if speaker_id != '0' else np.zeros(len(vector)) for speaker_id in permutation]\n",
    "        models_weigths = [len(recording_data['speakers_indexes'][speaker_id]) if speaker_id != '0' else 1 for speaker_id in permutation]\n",
    "        models_weigths_sum = np.sum(models_weigths)\n",
    "        models_weigths = np.ones(len(models_weigths)) - models_weigths / models_weigths_sum\n",
    "        \n",
    "        x = [vector] + models_container\n",
    "        y = torch.Tensor([speaker_id == target_id for speaker_id in permutation]).to(device, non_blocking = True).float()\n",
    "        z = models_weigths\n",
    "        \n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "for recording_id in recordings_segments_cut:\n",
    "    recordings_dataset = Recordings_dataset(recordings_segments_cut, recording_id)\n",
    "    print(recordings_dataset[5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
