{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: 249/249\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def is_valid_segment(segment):\n",
    "    return len(segment['speakers']) == 1 \\\n",
    "            and len(segment['ivectors']) == 1 \\\n",
    "            and len(segment['xvectors']) == 1 \\\n",
    "            and segment['speakers'][0]['speaker_id'] in ['A', 'B']\n",
    "\n",
    "directory = '../exp/json'\n",
    "filenames = [filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n",
    "\n",
    "recordings_segments = {}\n",
    "recordings_length = len(filenames)\n",
    "recordings_count = 0\n",
    "for filename in filenames:\n",
    "    recording_id = filename.split('.')[0]\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    file = open(filepath, 'r')\n",
    "    recordings_segments[recording_id] = [json.loads(line) for line in file.readlines()]\n",
    "    file.close()\n",
    "    recordings_segments[recording_id] = list(filter(is_valid_segment, recordings_segments[recording_id]))\n",
    "    recordings_count += 1\n",
    "    print('Loading data: ' + str(recordings_count) + '/' + str(recordings_length), end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recordings left: 172\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def speakers_get_indexes(accumulator, speaker_tuple):\n",
    "    speaker_id, index = speaker_tuple\n",
    "    if speaker_id in accumulator:\n",
    "        accumulator[speaker_id].append(index)\n",
    "    else:\n",
    "        accumulator[speaker_id] = [index]\n",
    "    return accumulator\n",
    "\n",
    "recordings_segments_cut = {}\n",
    "for recording_id in recordings_segments:\n",
    "    recording_segments = recordings_segments[recording_id]\n",
    "    speakers_indexes = [(segment['speakers'][0]['speaker_id'], index) for index, segment in enumerate(recording_segments)]\n",
    "    speakers_indexes = reduce(speakers_get_indexes, speakers_indexes, {})\n",
    "    speakers_lengths = [(speaker_id, len(speakers_indexes[speaker_id])) for speaker_id in speakers_indexes]\n",
    "    speakers_lengths.sort(key = lambda x: x[1])\n",
    "    speakers_lengths_min = speakers_lengths[0][1]\n",
    "    if len(speakers_lengths) > 1 and speakers_lengths_min >= 20: # <-- IMPORTANT\n",
    "        recording_indexes = []\n",
    "        for speaker_id in speakers_indexes:\n",
    "            speakers_indexes[speaker_id] = speakers_indexes[speaker_id][:speakers_lengths_min]\n",
    "            recording_indexes += speakers_indexes[speaker_id]\n",
    "        recordings_segments_cut[recording_id] = [segment for index, segment in enumerate(recordings_segments[recording_id]) if index in recording_indexes]\n",
    "print('Recordings left:', len(recordings_segments_cut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "models_generation_length = 3 # <-- IMPORTANT\n",
    "models_container_length = 2  # <-- IMPORTANT\n",
    "permutations_include_zeros = False\n",
    "\n",
    "class Recordings_dataset(Dataset):\n",
    "    def __init__(self, recordings_segments, recordings_ids, mode = 'ivectors'):\n",
    "        self.recordings_ids = recordings_ids if isinstance(recordings_ids, list) else [recordings_ids]\n",
    "        self.recordings_segments = {}\n",
    "        for recording_id in self.recordings_ids:\n",
    "            self.recordings_segments[recording_id] = recordings_segments[recording_id]\n",
    "        self.mode = mode\n",
    "        self.models_generation_length = models_generation_length\n",
    "        self.models_container_length = models_container_length\n",
    "        self.permutations_include_zeros = permutations_include_zeros\n",
    "        self.recordings_data = {}\n",
    "        self.recordings_map = []\n",
    "        self.recordings_length = 0\n",
    "        for recording_id in self.recordings_ids:\n",
    "            self.recordings_data[recording_id] = {}\n",
    "            recording_segments = self.recordings_segments[recording_id]\n",
    "            recording_data = self.recordings_data[recording_id]\n",
    "            recording_data['speakers_indexes'] = [(segment['speakers'][0]['speaker_id'], index) for index, segment in enumerate(recording_segments)]\n",
    "            recording_data['speakers_indexes'] = reduce(speakers_get_indexes, recording_data['speakers_indexes'], {})\n",
    "            recording_data['speakers_indexes_lengths_max'] = max([len(recording_data['speakers_indexes'][speaker_id]) for speaker_id in recording_data['speakers_indexes']])\n",
    "            recording_data['speakers_models'] = {}\n",
    "            for speaker_id in recording_data['speakers_indexes']:\n",
    "                speaker_indexes = recording_data['speakers_indexes'][speaker_id]\n",
    "                speaker_vectors = [np.asarray(recording_segments[index][self.mode][0]['value']) for index in speaker_indexes[:self.models_generation_length]]\n",
    "                recording_data['speakers_models'][speaker_id] = [np.sum(speaker_vectors, 0) / len(speaker_vectors)]\n",
    "            if self.permutations_include_zeros:\n",
    "                recording_data['permutations'] = list(itertools.permutations(list(recording_data['speakers_models'].keys()) \\\n",
    "                + ['0' for i in range(self.models_container_length)], self.models_container_length))\n",
    "            else:\n",
    "                recording_data['permutations'] = list(itertools.permutations(list(recording_data['speakers_models'].keys()), self.models_container_length))\n",
    "            recording_data['permutations'] = list(set(recording_data['permutations']))\n",
    "            recording_data['permutations'].sort()\n",
    "            recording_data['permutations_map'] = []\n",
    "            recording_data['permutations_length'] = 0\n",
    "            for index, permutation in enumerate(recording_data['permutations']):\n",
    "                speakers_models_length = int(np.prod([len(recording_data['speakers_models'][speaker_id]) for speaker_id in permutation if speaker_id != '0']))\n",
    "                recording_data['permutations_map'].append((recording_data['permutations_length'], recording_data['permutations_length'] + speakers_models_length - 1, index))\n",
    "                recording_data['permutations_length'] += speakers_models_length\n",
    "            recording_data['length'] = len(recording_segments) * recording_data['permutations_length']\n",
    "            self.recordings_map.append((self.recordings_length, self.recordings_length + recording_data['length'] - 1, recording_id))\n",
    "            self.recordings_length += recording_data['length']\n",
    "    def __len__(self):\n",
    "        return self.recordings_length\n",
    "    def __getitem__(self, idx):\n",
    "        recording_tuple = list(filter(lambda recording_tuple: recording_tuple[0] <= idx and idx <= recording_tuple[1], self.recordings_map))[0]\n",
    "        recording_idx = idx - recording_tuple[0]\n",
    "        recording_id = recording_tuple[2]\n",
    "        recording_data = self.recordings_data[recording_id]\n",
    "        \n",
    "        segment_id, segment_idx = divmod(recording_idx, recording_data['permutations_length'])\n",
    "        segment = self.recordings_segments[recording_id][segment_id]\n",
    "        target_id = segment['speakers'][0]['speaker_id']\n",
    "        vector = np.asarray(segment[self.mode][0]['value'])\n",
    "        \n",
    "        permutation_tuple = list(filter(lambda permutation_tuple: permutation_tuple[0] <= segment_idx and segment_idx <= permutation_tuple[1], recording_data['permutations_map']))[0]\n",
    "        permutation_id = permutation_tuple[2]\n",
    "        permutation = recording_data['permutations'][permutation_id]\n",
    "        \n",
    "        models_container = [np.asarray(recording_data['speakers_models'][speaker_id][0]) if speaker_id != '0' else np.zeros(len(vector)) for speaker_id in permutation]\n",
    "        models_weigths = np.asarray([len(recording_data['speakers_indexes'][speaker_id]) if speaker_id != '0' else recording_data['speakers_indexes_lengths_max'] for speaker_id in permutation])\n",
    "        models_weigths_sum = np.sum(models_weigths)\n",
    "        models_weigths = np.ones(len(models_weigths)) - models_weigths / models_weigths_sum\n",
    "        \n",
    "        x = [vector] + models_container\n",
    "        y = np.asarray([speaker_id == target_id for speaker_id in permutation], dtype = float)\n",
    "        z = models_weigths\n",
    "        \n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Live_graph:\n",
    "    def __init__(self, validation_threshold):\n",
    "        self.plt_count = -1\n",
    "        self.validation_threshold = validation_threshold\n",
    "        self.plt_thr = ([self.plt_count], [self.validation_threshold])\n",
    "        self.plt_loss = ([self.plt_count], [1])\n",
    "        self.plt_valid = ([self.plt_count], [1])\n",
    "        self.plt_test = ([self.plt_count], [1])\n",
    "        self.fig = plt.figure()\n",
    "        self.ax = self.fig.add_subplot()\n",
    "        self.line0, = self.ax.plot(self.plt_thr[0], self.plt_thr[1], 'k--', label = 'Threshold') # Threshold line\n",
    "        self.line1, = self.ax.plot(self.plt_loss[0], self.plt_loss[1], '--', label = 'Training') # Training loss\n",
    "        self.line2, = self.ax.plot(self.plt_valid[0], self.plt_valid[1], label = 'Validation')   # Validation loss\n",
    "        self.line3, = self.ax.plot(self.plt_test[0], self.plt_test[1], label = 'Test')           # Test loss\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        self.ax.legend()\n",
    "        self.ax.set_xlim(-1, 0)\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "    def step(self, training, validation, test):\n",
    "        self.plt_count += 1\n",
    "        self.plt_thr[0].append(self.plt_count)\n",
    "        self.plt_thr[1].append(self.validation_threshold)\n",
    "        self.plt_loss[0].append(self.plt_count)\n",
    "        self.plt_loss[1].append(training)\n",
    "        self.plt_valid[0].append(self.plt_count)\n",
    "        self.plt_valid[1].append(validation)\n",
    "        self.plt_test[0].append(self.plt_count)\n",
    "        self.plt_test[1].append(test)\n",
    "        self.line0.set_xdata(self.plt_thr[0])\n",
    "        self.line0.set_ydata(self.plt_thr[1])\n",
    "        self.line1.set_xdata(self.plt_loss[0])\n",
    "        self.line1.set_ydata(self.plt_loss[1])\n",
    "        self.line2.set_xdata(self.plt_valid[0])\n",
    "        self.line2.set_ydata(self.plt_valid[1])\n",
    "        self.line3.set_xdata(self.plt_test[0])\n",
    "        self.line3.set_ydata(self.plt_test[1])\n",
    "        self.ax.set_xlim(0, self.plt_count + 1)\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n = models_container_length\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d((n + 1), (n + 1), 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.2),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear((n + 1) * 128, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(16, n),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = torch.stack(input, 1)\n",
    "        x = self.cnn1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('Running on the GPU.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Running on the CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def md_eval(ref_filepath, res_filepath):\n",
    "    bin = '../../../../tools/sctk-2.4.10/src/md-eval/md-eval.pl'\n",
    "    p = subprocess.Popen([bin, '-r', ref_filepath, '-s', res_filepath], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    output, err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    if rc == 0:\n",
    "        lines =  output.split('\\n')\n",
    "        derLine = [line for line in lines if 'OVERALL SPEAKER DIARIZATION ERROR' in line][0]\n",
    "        return float(re.findall('\\d+\\.\\d+', derLine)[0])\n",
    "    else:\n",
    "        exit('md-eval.pl fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size = 1, num_workers = 8)\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input, target, weigth in dataloader:\n",
    "            input = [tensor.to(device, non_blocking = True).float() for tensor in input]\n",
    "            target = target.to(device, non_blocking = True).float()\n",
    "            weigth = weigth.to(device, non_blocking = True).float()\n",
    "            output = net(input)\n",
    "            if target.max(1)[1] == output.max(1)[1]:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "            print('test: ' + str(count) + '/' + str(len(dataloader)) + ' accuracy: ' + str(correct / count), end = '\\r')\n",
    "        print('test: ' + str(count) + '/' + str(len(dataloader)) + ' accuracy: ' + str(correct / count))\n",
    "        \n",
    "def test_diarization(net, recordings_ids):\n",
    "    results = ''\n",
    "    for recording_id in recordings_ids:\n",
    "        recording_dataset = Recordings_dataset(recordings_segments_cut, recording_id)\n",
    "        speakers_models = recording_dataset.recordings_data[recording_id]['speakers_models']\n",
    "        # At this point there is no information about the speaker identity, only the model\n",
    "        speakers_models = [speakers_models[speaker_id][0] for speaker_id in speakers_models]\n",
    "        last_speaker_index = -1\n",
    "        last_speaker = { 'begining': 0, 'ending': 0, 'index': -1 }\n",
    "        for segment in recordings_segments[recording_id]:\n",
    "            begining = segment['begining']\n",
    "            ending = segment['ending']\n",
    "            vector = np.asarray(segment['ivectors'][0]['value'])\n",
    "            with torch.no_grad():\n",
    "                input = [torch.Tensor([nparray]).to(device, non_blocking = True).float() for nparray in [vector] + speakers_models]\n",
    "                output = net(input)\n",
    "                index = output.max(1)[1].cpu().data.numpy()[0]\n",
    "                if last_speaker_index != index:\n",
    "                    if last_speaker_index != -1:\n",
    "                        result = 'SPEAKER ' + recording_id + ' 0 ' + str(last_speaker['begining']) + ' ' + str(round(last_speaker['ending'] - last_speaker['begining'], 2)) + ' <NA> <NA> ' + str(last_speaker['index']) + ' <NA> <NA>'\n",
    "                        results += result + '\\n'\n",
    "                    last_speaker_index = index\n",
    "                    last_speaker = { 'begining': begining, 'ending': ending, 'index': index }\n",
    "                else:\n",
    "                    if begining <= last_speaker['ending']:\n",
    "                        last_speaker['ending'] = ending\n",
    "                    else:\n",
    "                        if last_speaker_index != -1:\n",
    "                            result = 'SPEAKER ' + recording_id + ' 0 ' + str(last_speaker['begining']) + ' ' + str(round(last_speaker['ending'] - last_speaker['begining'], 2)) + ' <NA> <NA> ' + str(last_speaker['index']) + ' <NA> <NA>'\n",
    "                            results += result + '\\n'\n",
    "                        last_speaker_index = index\n",
    "                        last_speaker = { 'begining': begining, 'ending': ending, 'index': index }\n",
    "                        \n",
    "\n",
    "    groundtruth_rttm_filepath = '../callhome1_1.0_0.5.rttm'\n",
    "    file = open(groundtruth_rttm_filepath, 'r')\n",
    "    test_rttm = ''.join([line for line in file.readlines() if (line.split(' ')[1] in recordings_ids) and \\\n",
    "                    (line.split(' ')[7] in ['A', 'B'])])\n",
    "    file.close()\n",
    "\n",
    "    file = open('test_results.rttm', 'w')\n",
    "    file.write(results)\n",
    "    file.close()\n",
    "\n",
    "    file = open('test_groundtruth.rttm', 'w')\n",
    "    file.write(test_rttm)\n",
    "    file.close()\n",
    "    \n",
    "    print(md_eval('test_groundtruth.rttm', 'test_results.rttm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../tools/sctk-2.4.10/src/md-eval/md-eval.pl': '../../../../tools/sctk-2.4.10/src/md-eval/md-eval.pl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-db8c3c14ecaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m#test_accuracy(net, test_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mtest_diarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mfold_begining\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfold_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-9786dfd9af5a>\u001b[0m in \u001b[0;36mtest_diarization\u001b[0;34m(net, recordings_ids)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_groundtruth.rttm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_results.rttm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-9509343040b7>\u001b[0m in \u001b[0;36mmd_eval\u001b[0;34m(ref_filepath, res_filepath)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmd_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../../../tools/sctk-2.4.10/src/md-eval/md-eval.pl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_filepath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1362\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../tools/sctk-2.4.10/src/md-eval/md-eval.pl': '../../../../tools/sctk-2.4.10/src/md-eval/md-eval.pl'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "recordings_ids = [recording_id for recording_id in recordings_segments_cut]\n",
    "folds = 4\n",
    "fold_length = int(len(recordings_ids) / folds)\n",
    "fold_begining = 0\n",
    "for fold in range(folds):\n",
    "    fold_finish = fold_begining + fold_length - 1\n",
    "    train_ids = recordings_ids[:fold_begining] + recordings_ids[fold_finish:]\n",
    "    test_ids = recordings_ids[fold_begining:fold_finish]\n",
    "    \n",
    "    train_dataset = Recordings_dataset(recordings_segments_cut, train_ids)\n",
    "\n",
    "    train_length = int(len(train_dataset) * 0.7)\n",
    "    valid_length = len(train_dataset) - train_length\n",
    "    \n",
    "    train_dataset, valid_dataset = random_split(train_dataset, [train_length, valid_length])\n",
    "    test_dataset = Recordings_dataset(recordings_segments_cut, test_ids)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = 10, shuffle=True, num_workers = 8)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size = 10, num_workers = 8)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = 10, num_workers = 8)\n",
    "    \n",
    "    '''# ----- Training ----- #\n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.0004) # 0.0004 GOOD\n",
    "\n",
    "    epochs = 10\n",
    "    validation_threshold = 0.1\n",
    "\n",
    "    live_graph = Live_graph(validation_threshold)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        for input, target, weigth in train_dataloader:\n",
    "            input = [tensor.to(device, non_blocking = True).float() for tensor in input]\n",
    "            target = target.to(device, non_blocking = True).float()\n",
    "            weigth = weigth.to(device, non_blocking = True).float()\n",
    "\n",
    "            criterion = nn.BCELoss(weigth)\n",
    "            net.zero_grad()\n",
    "            output = net(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.data)\n",
    "            print('train: ' + str(len(train_losses)) + '/' + str(len(train_dataloader)) + '     ', end = '\\r')\n",
    "        train_loss = np.sum(train_losses) / len(train_losses)\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            validation_losses = []\n",
    "            for input, target, weigth in valid_dataloader:\n",
    "                input = [tensor.to(device, non_blocking = True).float() for tensor in input]\n",
    "                target = target.to(device, non_blocking = True).float()\n",
    "                weigth = weigth.to(device, non_blocking = True).float()\n",
    "\n",
    "                criterion = nn.BCELoss(weigth)\n",
    "                output = net(input)\n",
    "                loss = criterion(output, target)\n",
    "                validation_losses.append(loss.data)\n",
    "                print('validation: ' + str(len(validation_losses)) + '/' + str(len(valid_dataloader)) + '     ', end = '\\r')\n",
    "            validation_loss = np.sum(validation_losses) / len(validation_losses)\n",
    "\n",
    "            test_losses = []\n",
    "            for input, target, weigth in test_dataloader:\n",
    "                input = [tensor.to(device, non_blocking = True).float() for tensor in input]\n",
    "                target = target.to(device, non_blocking = True).float()\n",
    "                weigth = weigth.to(device, non_blocking = True).float()\n",
    "\n",
    "                criterion = nn.BCELoss(weigth)\n",
    "                output = net(input)\n",
    "                loss = criterion(output, target)\n",
    "                test_losses.append(loss.data)\n",
    "                print('test: ' + str(len(test_losses)) + '/' + str(len(test_dataloader)) + '     ', end = '\\r')\n",
    "            test_loss = np.sum(test_losses) / len(test_losses)\n",
    "\n",
    "        live_graph.step(train_loss, validation_loss, test_loss)\n",
    "\n",
    "        if validation_loss <= validation_threshold:\n",
    "            print('Done training.')\n",
    "            break\n",
    "    # ----- Training ----- #'''\n",
    "    \n",
    "    #test_accuracy(net, test_dataset)\n",
    "    test_diarization(net, test_ids)\n",
    "    \n",
    "    fold_begining += fold_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
