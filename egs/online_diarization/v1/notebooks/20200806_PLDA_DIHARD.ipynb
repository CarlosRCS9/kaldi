{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordings segments loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "# is_single_speaker_segment [VALIDATED]\n",
    "# validates if a segment has a single speaker who belongs to the speakers list. \n",
    "def is_single_speaker_segment(segment, valid_speakers_ids = ['A', 'B']):\n",
    "    return len(segment['speakers']) == 1 and segment['speakers'][0]['speaker_id'] in valid_speakers_ids\n",
    "\n",
    "# is_valid_segment [VALIDATED]\n",
    "# validates if a segment meets a maximum number of speakers,\n",
    "# and that all the speakers in the segment belong to a list.\n",
    "def is_valid_segment(segment, maximum_speakers_length = 2, valid_speakers_ids = ['A', 'B']):\n",
    "    speakers_ids = [speaker['speaker_id'] for speaker in segment['speakers']]\n",
    "    speakers_ids = list(set(speakers_ids))\n",
    "    return len(speakers_ids) <= maximum_speakers_length and \\\n",
    "        all(speaker_id in valid_speakers_ids for speaker_id in speakers_ids)\n",
    "\n",
    "# load_recordings_segments [VALIDATED]\n",
    "# loads the recordings segments data from the .json files located in a directory \n",
    "def load_recordings_segments(directory, validation_function):\n",
    "    filenames = [filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))]\n",
    "    filenames.sort()\n",
    "    recordings_segments = {}\n",
    "    recordings_length = len(filenames)\n",
    "    recordings_count = 0\n",
    "    segments_original = 0\n",
    "    segments_filtered = 0\n",
    "    for filename in filenames:\n",
    "        recording_id = filename.split('.')[0]\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        file = open(filepath, 'r')\n",
    "        recordings_segments[recording_id] = [json.loads(line) for line in file.readlines()]\n",
    "        file.close()\n",
    "        segments_original += len(recordings_segments[recording_id])\n",
    "        recordings_segments[recording_id] = list(filter(validation_function, recordings_segments[recording_id]))\n",
    "        segments_filtered += len(recordings_segments[recording_id])\n",
    "        recordings_count += 1\n",
    "        print(directory + ' loading ' + str(recordings_count) + '/' + str(recordings_length), end = '\\r')\n",
    "    print(directory, 'loaded', str(recordings_count) + '/' + str(recordings_length) + ',', round(segments_filtered / segments_original, 2), 'segments left.')\n",
    "    return recordings_segments\n",
    "\n",
    "# speakers_get_indexes [VALIDATED]\n",
    "# used to convert a (speakers_ids, index) list to a speakers_ids => [indexes] dictionary\n",
    "def speakers_get_indexes(accumulator, speakers_tuple):\n",
    "    speaker_ids, index = speakers_tuple\n",
    "    speaker_ids = ','.join(speaker_ids)\n",
    "    if speaker_ids in accumulator:\n",
    "        accumulator[speaker_ids].append(index)\n",
    "    else:\n",
    "        accumulator[speaker_ids] = [index]\n",
    "    return accumulator\n",
    "\n",
    "# balance_segments [VALIDATED]\n",
    "# balances the recording segments data to meet a minimum of speakers per recording,\n",
    "# and a minimum of segments per speaker.\n",
    "def balance_segments(recordings_segments,\n",
    "                     minimum_speakers_length = 2,\n",
    "                     minimum_speaker_segments = 3,\n",
    "                     include_overlaps = False):\n",
    "    new_recordings_segments = {}\n",
    "    for recording_id in recordings_segments:\n",
    "        recording_segments = recordings_segments[recording_id]\n",
    "        # ----- Obtaining speakers indexes ----- #\n",
    "        speakers_indexes = [(sorted(list(set([speaker['speaker_id'] for speaker in segment['speakers']]))), index) for index, segment in enumerate(recording_segments)]\n",
    "        speakers_indexes = reduce(speakers_get_indexes, speakers_indexes, {})\n",
    "        # ----- Removing overlaps ----- #\n",
    "        if not include_overlaps:\n",
    "            for speakers_ids in list(speakers_indexes.keys()):\n",
    "                if len(speakers_ids.split(',')) > 1:\n",
    "                    del speakers_indexes[speakers_ids]\n",
    "        speakers_lengths = [(speakers_ids, len(speakers_indexes[speakers_ids])) for speakers_ids in speakers_indexes]\n",
    "        speakers_lengths.sort(key = lambda x: x[1])\n",
    "        speakers_lengths_min = speakers_lengths[0][1]\n",
    "        if len(speakers_lengths) >= minimum_speakers_length and speakers_lengths_min >= minimum_speaker_segments:\n",
    "            recording_indexes = []\n",
    "            for speakers_ids in speakers_indexes:\n",
    "                speakers_indexes[speakers_ids] = speakers_indexes[speakers_ids][:speakers_lengths_min]\n",
    "                recording_indexes += speakers_indexes[speakers_ids]\n",
    "            new_recordings_segments[recording_id] = [segment for index, segment in enumerate(recordings_segments[recording_id]) if index in recording_indexes]\n",
    "    print('Recordings left: ' + str(len(new_recordings_segments)) + '/' + str(len(recordings_segments)))\n",
    "    return new_recordings_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def generate_speaker_model(speakers_segments_indexes,\n",
    "                           recordings_segments,\n",
    "                           segments_length,\n",
    "                           vector = 'ivectors',\n",
    "                           selection = 'first',\n",
    "                           indexes = []):\n",
    "    if selection == 'first':\n",
    "        selected_segments = [recordings_segments[recording_id][index] for recording_id, index, real in speakers_segments_indexes[:segments_length]]\n",
    "    elif selection == 'random':\n",
    "        selected_segments = [recordings_segments[recording_id][index] for recording_id, index, real in random.sample(speakers_segments_indexes, segments_length if segments_length < len(speakers_segments_indexes) else len(speakers_segments_indexes))]\n",
    "    else:\n",
    "        print('ERROR: unknown speaker model segments selection strategy.')\n",
    "    selected_vectors = [np.asarray(segment[vector][0]['value']) for segment in selected_segments]\n",
    "    return np.sum(selected_vectors, 0) / len(selected_vectors)\n",
    "\n",
    "def get_speakers_segments_indexes(acc, recording_id_index_real, recordings_segments):\n",
    "    recording_id, index, real = recording_id_index_real\n",
    "    segment = recordings_segments[recording_id][index]\n",
    "    speakers_ids = ','.join(sorted([speaker['speaker_id'] for speaker in segment['speakers']]))\n",
    "    if speakers_ids not in acc:\n",
    "        acc[speakers_ids] = []\n",
    "    acc[speakers_ids].append(recording_id_index_real)\n",
    "    return acc\n",
    "\n",
    "class Recordings_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 recordings_segments,\n",
    "                 recordings_ids = None,\n",
    "                 vector = 'ivectors',\n",
    "                 models_container_length = 2,\n",
    "                 models_container_include_zeros = True,\n",
    "                 models_container_include_overlaps = False,\n",
    "                 models_generation_lengths = [3],\n",
    "                 models_generation_selection = 'first',\n",
    "                 include_false_segments = 'models'):\n",
    "        # ----- Saving the recordings ids ----- #\n",
    "        if recordings_ids is None:\n",
    "            recordings_ids = [recording_id for recording_id in recordings_segments]\n",
    "        self.recordings_ids = recordings_ids if isinstance(recordings_ids, list) else [recordings_ids] \n",
    "        # ----- Saving the recordings segments ----- #\n",
    "        self.recordings_segments = recordings_segments\n",
    "        # ----- Saving the arguments ----- #\n",
    "        self.vector = vector\n",
    "        self.models_container_length = models_container_length\n",
    "        self.models_container_include_zeros = models_container_include_zeros\n",
    "        self.models_container_include_overlaps = models_container_include_overlaps\n",
    "        self.models_generation_lengths = models_generation_lengths\n",
    "        self.models_generation_selection = models_generation_selection\n",
    "        self.include_false_segments = include_false_segments\n",
    "        \n",
    "        self.recordings_data = {}\n",
    "        self.recordings_models = {}\n",
    "        \n",
    "        # ----- Initializing the recordings data ----- #\n",
    "        for recording_id in self.recordings_ids:\n",
    "            self.recordings_data[recording_id] = {}\n",
    "        \n",
    "        # ----- Saving the recordings segments indexes ----- #\n",
    "        for recording_id in self.recordings_data:\n",
    "            recording_data = self.recordings_data[recording_id]\n",
    "            recording_data['recording_segments_indexes'] = [(recording_id, index, True) for index, segment in enumerate(self.recordings_segments[recording_id])]\n",
    "            \n",
    "        # ----- Obtaining the recordings speakers segments indexes ----- #\n",
    "        for recording_id in self.recordings_data:\n",
    "            recording_data = self.recordings_data[recording_id]\n",
    "            recording_data['speakers_segments_indexes'] = reduce(lambda acc, recording_id_index_real: get_speakers_segments_indexes(acc, recording_id_index_real, self.recordings_segments), recording_data['recording_segments_indexes'], {})\n",
    "            # ----- Obtaining the max speaker segments length ----- #\n",
    "            recording_data['speakers_segments_indexes_lengths_max'] = max([len(recording_data['speakers_segments_indexes'][speakers_ids]) for speakers_ids in recording_data['speakers_segments_indexes']])\n",
    "      \n",
    "        # ----- Generating the recordings speakers models ----- #\n",
    "        for recording_id in self.recordings_data:\n",
    "            self.recordings_models[recording_id] = []\n",
    "            recording_data = self.recordings_data[recording_id]\n",
    "            recording_data['speakers_models'] = {}\n",
    "            for speakers_ids in recording_data['speakers_segments_indexes']:\n",
    "                recording_data['speakers_models'][speakers_ids] = {}\n",
    "                for models_generation_length in models_generation_lengths:\n",
    "                    speakers_model = generate_speaker_model(recording_data['speakers_segments_indexes'][speakers_ids], self.recordings_segments, models_generation_length, self.vector, self.models_generation_selection)\n",
    "                    model_segment = {}\n",
    "                    model_segment[vector] = [{ 'value': speakers_model }]\n",
    "                    model_segment['speakers'] = [{ 'speaker_id': speakers_ids }]\n",
    "                    self.recordings_models[recording_id].append(model_segment)\n",
    "                    recording_data['speakers_models'][speakers_ids][models_generation_length] = [speakers_model]\n",
    "        \n",
    "        # ----- Generating the recordings speakers models permutations ----- #\n",
    "        for recording_id in self.recordings_data:\n",
    "            recording_data = self.recordings_data[recording_id]\n",
    "            if self.models_container_include_zeros:\n",
    "                recording_data['permutations'] = list(itertools.permutations(list(recording_data['speakers_models'].keys()) + ['0' for _ in range(self.models_container_length)], self.models_container_length))\n",
    "            else:\n",
    "                recording_data['permutations'] = list(itertools.permutations(list(recording_data['speakers_models'].keys()), self.models_container_length))\n",
    "            recording_data['permutations'] = sorted(set(recording_data['permutations']))\n",
    "            if not self.models_container_include_overlaps:\n",
    "                recording_data['permutations'] = [permutation for permutation in recording_data['permutations'] if all(len(speakers_ids.split(',')) == 1 for speakers_ids in permutation)]\n",
    "\n",
    "        # ----- Generatign false segments ----- #\n",
    "        if self.include_false_segments == 'segments':\n",
    "            for recording_id in self.recordings_data:\n",
    "                recording_data = self.recordings_data[recording_id]\n",
    "                other_recordings_segments_indexes = []\n",
    "                for other_recording_id in self.recordings_segments:\n",
    "                    if other_recording_id != recording_id:\n",
    "                        other_recordings_segments_indexes += [(other_recording_id, index, False) for index, segment in enumerate(self.recordings_segments[other_recording_id])]\n",
    "                if len(other_recordings_segments_indexes) >= recording_data['speakers_segments_indexes_lengths_max']:\n",
    "                    other_recordings_segments_indexes = random.sample(other_recordings_segments_indexes, recording_data['speakers_segments_indexes_lengths_max'])\n",
    "                    options = [recording_data['recording_segments_indexes'], other_recordings_segments_indexes]\n",
    "                    options_lengths = [len(option) for option in options]\n",
    "                    new_recording_segments_indexes = []\n",
    "                    while sum(options_lengths) > 0:\n",
    "                        options_indexes = list(itertools.chain(*[[index] * len(option) for index, option in enumerate(options)]))\n",
    "                        option_index = random.choice(options_indexes)\n",
    "                        new_recording_segments_indexes.append(options[option_index].pop(0))\n",
    "                        options_lengths = [len(option) for option in options]\n",
    "                    recording_data['recording_segments_indexes'] = new_recording_segments_indexes\n",
    "        elif self.include_false_segments == 'models':\n",
    "            for recording_id in self.recordings_data:\n",
    "                recording_data = self.recordings_data[recording_id]\n",
    "                other_recordings_segments_indexes = []\n",
    "                for other_recording_id in self.recordings_models:\n",
    "                    if other_recording_id != recording_id:\n",
    "                        other_recordings_segments_indexes += [(other_recording_id, index, False) for index, segment in enumerate(self.recordings_models[other_recording_id])]\n",
    "                other_recordings_segments_indexes = random.sample(other_recordings_segments_indexes, int(0.5 * recording_data['speakers_segments_indexes_lengths_max'] if recording_data['speakers_segments_indexes_lengths_max'] < len(other_recordings_segments_indexes) else len(other_recordings_segments_indexes)))\n",
    "                \n",
    "                options = [recording_data['recording_segments_indexes'], other_recordings_segments_indexes]\n",
    "                options_lengths = [len(option) for option in options]\n",
    "                new_recording_segments_indexes = []\n",
    "                while sum(options_lengths) > 0:\n",
    "                    options_indexes = list(itertools.chain(*[[index] * len(option) for index, option in enumerate(options)]))\n",
    "                    option_index = random.choice(options_indexes)\n",
    "                    new_recording_segments_indexes.append(options[option_index].pop(0))\n",
    "                    options_lengths = [len(option) for option in options]\n",
    "                recording_data['recording_segments_indexes'] = new_recording_segments_indexes\n",
    "                \n",
    "        # ----- Obtaining the dataset length ----- #\n",
    "        self.recordings_length = 0\n",
    "        self.recordings_map = []\n",
    "        for recording_id in self.recordings_data:\n",
    "            recording_data = self.recordings_data[recording_id]\n",
    "            recording_data['permutations_map'] = []\n",
    "            recording_data['permutations_length'] = 0\n",
    "            for index, permutation in enumerate(recording_data['permutations']):\n",
    "                speakers_models_length = int(np.prod([np.sum([len(recording_data['speakers_models'][speakers_ids][models_generation_length]) for models_generation_length in recording_data['speakers_models'][speakers_ids]]) for speakers_ids in permutation if speakers_ids != '0']))\n",
    "                recording_data['permutations_map'].append((recording_data['permutations_length'], recording_data['permutations_length'] + speakers_models_length - 1, index))\n",
    "                recording_data['permutations_length'] += speakers_models_length\n",
    "            recording_data['length'] = len(recording_data['recording_segments_indexes']) * recording_data['permutations_length']\n",
    "            self.recordings_map.append((self.recordings_length, self.recordings_length + recording_data['length'] - 1, recording_id))\n",
    "            self.recordings_length += recording_data['length']\n",
    "    def __len__(self):\n",
    "        return self.recordings_length\n",
    "    def __getitem__(self, idx):\n",
    "        recording_limits = list(filter(lambda recording_limits: recording_limits[0] <= idx and idx <= recording_limits[1], self.recordings_map))[0]\n",
    "        recording_idx = idx - recording_limits[0]\n",
    "        recording_id = recording_limits[2]\n",
    "        recording_data = self.recordings_data[recording_id]\n",
    "        \n",
    "        segment_id, segment_idx = divmod(recording_idx, recording_data['permutations_length'])\n",
    "        segment_recording_id, segment_index, segment_real = recording_data['recording_segments_indexes'][segment_id]\n",
    "        if not segment_real and self.include_false_segments == 'models':\n",
    "            segment = self.recordings_models[segment_recording_id][segment_index]\n",
    "        else:\n",
    "            segment = self.recordings_segments[segment_recording_id][segment_index]\n",
    "        vector = np.asarray(segment[self.vector][0]['value'])\n",
    "        \n",
    "        permutation_limits = list(filter(lambda permutation_limits: permutation_limits[0] <= segment_idx and segment_idx <= permutation_limits[1], recording_data['permutations_map']))[0]\n",
    "        permutation_idx = segment_idx - permutation_limits[0]\n",
    "        permutation_index = permutation_limits[2]\n",
    "        permutation = recording_data['permutations'][permutation_index]\n",
    "        \n",
    "        speakers_models_lengths = [np.sum([len(recording_data['speakers_models'][speakers_ids][models_generation_length]) for models_generation_length in recording_data['speakers_models'][speakers_ids]])  if speakers_ids != '0' else 1 for speakers_ids in permutation]\n",
    "        models_container = []\n",
    "        model_index = permutation_idx\n",
    "        for i, length_i in enumerate(speakers_models_lengths):\n",
    "            if i != len(speakers_models_lengths) - 1:\n",
    "                model_index, remainder = divmod(model_index, np.sum(speakers_models_lengths[i + 1:]))\n",
    "            else:\n",
    "                model_index = remainder\n",
    "            models_container.append(recording_data['speakers_models'][permutation[i]][self.models_generation_lengths[model_index]][0] if permutation[i] != '0' else np.random.uniform(-0.1, 0.1, len(vector)))\n",
    "        \n",
    "        models_weigths = np.asarray([len(recording_data['speakers_segments_indexes'][speakers_ids]) if speakers_ids != '0' else recording_data['speakers_segments_indexes_lengths_max'] for speakers_ids in permutation])\n",
    "        models_weigths_sum = np.sum(models_weigths)\n",
    "        models_weigths = np.ones(len(models_weigths)) - models_weigths / models_weigths_sum\n",
    "        \n",
    "        targets_ids = [speaker['speaker_id'] for speaker in segment['speakers']]\n",
    "        \n",
    "        x = [vector] + models_container\n",
    "        if segment_real:\n",
    "            if self.models_container_include_overlaps:\n",
    "                targets_ids = ','.join(sorted(list(set(targets_ids))))\n",
    "                y = np.asarray([speakers_ids == targets_ids for speakers_ids in permutation], dtype = float)\n",
    "            else:\n",
    "                y = np.asarray([speaker_id in targets_ids for speaker_id in permutation], dtype = float) / len(targets_ids)\n",
    "        else:\n",
    "            y = np.zeros(len(permutation))\n",
    "        z = models_weigths\n",
    "\n",
    "        return x, y, z, segment_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load live_graph.py\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Live_graph:\n",
    "    def __init__(self, validation_threshold):\n",
    "        self.plt_count = -1\n",
    "        self.validation_threshold = validation_threshold\n",
    "        self.plt_thr = ([self.plt_count], [self.validation_threshold])\n",
    "        self.plt_loss = ([self.plt_count], [1])\n",
    "        self.plt_valid = ([self.plt_count], [1])\n",
    "        self.plt_test = ([self.plt_count], [1])\n",
    "        self.fig = plt.figure()\n",
    "        self.ax = self.fig.add_subplot()\n",
    "        self.line0, = self.ax.plot(self.plt_thr[0], self.plt_thr[1], 'k--', label = 'Threshold') # Threshold line\n",
    "        self.line1, = self.ax.plot(self.plt_loss[0], self.plt_loss[1], '--', label = 'Training') # Training loss\n",
    "        self.line2, = self.ax.plot(self.plt_valid[0], self.plt_valid[1], label = 'Validation')   # Validation loss\n",
    "        self.line3, = self.ax.plot(self.plt_test[0], self.plt_test[1], label = 'Test')           # Test loss\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        self.ax.legend()\n",
    "        self.ax.set_xlim(-1, 0)\n",
    "        self.ax.set_ylim(0, 0.5)\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "    def step(self, training, validation, test = -1):\n",
    "        self.plt_count += 1\n",
    "        self.plt_thr[0].append(self.plt_count)\n",
    "        self.plt_thr[1].append(self.validation_threshold)\n",
    "        self.plt_loss[0].append(self.plt_count)\n",
    "        self.plt_loss[1].append(training)\n",
    "        self.plt_valid[0].append(self.plt_count)\n",
    "        self.plt_valid[1].append(validation)\n",
    "        self.plt_test[0].append(self.plt_count)\n",
    "        self.plt_test[1].append(test)\n",
    "        self.line0.set_xdata(self.plt_thr[0])\n",
    "        self.line0.set_ydata(self.plt_thr[1])\n",
    "        self.line1.set_xdata(self.plt_loss[0])\n",
    "        self.line1.set_ydata(self.plt_loss[1])\n",
    "        self.line2.set_xdata(self.plt_valid[0])\n",
    "        self.line2.set_ydata(self.plt_valid[1])\n",
    "        self.line3.set_xdata(self.plt_test[0])\n",
    "        self.line3.set_ydata(self.plt_test[1])\n",
    "        self.ax.set_xlim(0, self.plt_count + 1)\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, models_container_length, vector_length):\n",
    "        super().__init__()\n",
    "        n = models_container_length\n",
    "        m = vector_length\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d((n + 1), n ** 3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(n ** 3, n ** 2, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(n ** 2, n, 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lstm1 = nn.Sequential(\n",
    "            nn.LSTM(m - 6, 32, bidirectional = True),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64 * n, n * 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n * 32, n * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n * 16, n),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = torch.stack(input, 1)\n",
    "        x = self.cnn1(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "    def get_net(self,\n",
    "                recordings_segments,\n",
    "                recordings_ids = None,\n",
    "                vector = 'ivectors',\n",
    "                vector_length = 128,\n",
    "                models_container_length = 2,\n",
    "                models_container_include_zeros = True,\n",
    "                models_container_include_overlaps = False,\n",
    "                models_generation_lengths = [3],\n",
    "                models_generation_selection = 'first',\n",
    "                balance_segments = True,\n",
    "                balance_segments_selection = 'copy',\n",
    "                batch_size = 16,\n",
    "                num_workers = 8,\n",
    "                test_recordings_segments = None):\n",
    "        \n",
    "        if recordings_ids is None:\n",
    "            recordings_ids = [recording_id for recording_id in recordings_segments]\n",
    "        self.recordings_ids = recordings_ids if isinstance(recordings_ids, list) else [recordings_ids]\n",
    "\n",
    "        '''train_dataset = Recordings_dataset(recordings_segments,\n",
    "                                           recordings_ids, \n",
    "                                           vector,\n",
    "                                           models_container_length,\n",
    "                                           models_container_include_zeros,\n",
    "                                           models_container_include_overlaps,\n",
    "                                           models_generation_lengths,\n",
    "                                           models_generation_selection)\n",
    "\n",
    "        train_length = int(len(train_dataset) * 0.7)\n",
    "        valid_length = len(train_dataset) - train_length\n",
    "\n",
    "        train_dataset, valid_dataset = random_split(train_dataset, [train_length, valid_length])\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers = num_workers)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, num_workers = num_workers)'''\n",
    "        \n",
    "        train_length = int(len(self.recordings_ids) * 0.7)\n",
    "        valid_length = len(self.recordings_ids) - train_length\n",
    "        \n",
    "        valid_recordings_ids = sorted(random.sample(self.recordings_ids, valid_length))\n",
    "        train_recordings_ids = [recording_id for recording_id in self.recordings_ids if recording_id not in valid_recordings_ids]\n",
    "\n",
    "        train_dataset = Recordings_dataset(recordings_segments,\n",
    "                                           train_recordings_ids, \n",
    "                                           vector,\n",
    "                                           models_container_length,\n",
    "                                           models_container_include_zeros,\n",
    "                                           models_container_include_overlaps,\n",
    "                                           models_generation_lengths,\n",
    "                                           models_generation_selection)\n",
    "        \n",
    "        valid_dataset = Recordings_dataset(recordings_segments,\n",
    "                                           valid_recordings_ids, \n",
    "                                           vector,\n",
    "                                           models_container_length,\n",
    "                                           models_container_include_zeros,\n",
    "                                           models_container_include_overlaps,\n",
    "                                           models_generation_lengths,\n",
    "                                           models_generation_selection)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers = num_workers)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, num_workers = num_workers)\n",
    "        \n",
    "        if test_recordings_segments is not None:\n",
    "            test_recordings_ids = [recording_id for recording_id in test_recordings_segments]\n",
    "            test_dataset = Recordings_dataset(test_recordings_segments,\n",
    "                                              test_recordings_ids,\n",
    "                                              vector,\n",
    "                                              models_container_length,\n",
    "                                              models_container_include_zeros,\n",
    "                                              models_container_include_overlaps,\n",
    "                                              models_generation_lengths,\n",
    "                                              models_generation_selection)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size = batch_size, num_workers = num_workers)\n",
    "\n",
    "        net = Net(models_container_length, vector_length).to(self.device)\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "        epochs = 20\n",
    "        validation_threshold = 0.05\n",
    "\n",
    "        live_graph = Live_graph(validation_threshold)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_losses = []\n",
    "            for input, target, weigth, segment_real in train_dataloader:\n",
    "                input = [tensor.to(self.device, non_blocking = True).float() for tensor in input]\n",
    "                target = target.to(self.device, non_blocking = True).float()\n",
    "                weigth = weigth.to(self.device, non_blocking = True).float()\n",
    "\n",
    "                criterion = nn.BCELoss(weigth)\n",
    "                net.zero_grad()\n",
    "                output = net(input)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses.append(loss.data)\n",
    "                print('train: ' + str(len(train_losses)) + '/' + str(len(train_dataloader)) + '          ', end = '\\r')\n",
    "            train_loss = np.sum(train_losses) / len(train_losses)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                validation_losses = []\n",
    "                for input, target, weigth, segment_real in valid_dataloader:\n",
    "                    input = [tensor.to(self.device, non_blocking = True).float() for tensor in input]\n",
    "                    target = target.to(self.device, non_blocking = True).float()\n",
    "                    weigth = weigth.to(self.device, non_blocking = True).float()\n",
    "\n",
    "                    criterion = nn.BCELoss(weigth)\n",
    "                    output = net(input)\n",
    "                    loss = criterion(output, target)\n",
    "                    validation_losses.append(loss.data)\n",
    "                    print('validation: ' + str(len(validation_losses)) + '/' + str(len(valid_dataloader)) + '          ', end = '\\r')\n",
    "                validation_loss = np.sum(validation_losses) / len(validation_losses)\n",
    "                \n",
    "                test_loss = -1\n",
    "                if test_recordings_segments is not None:\n",
    "                    test_losses = []\n",
    "                    for input, target, weigth in test_dataloader:\n",
    "                        input = [tensor.to(self.device, non_blocking = True).float() for tensor in input]\n",
    "                        target = target.to(self.device, non_blocking = True).float()\n",
    "                        weigth = weigth.to(self.device, non_blocking = True).float()\n",
    "\n",
    "                        criterion = nn.BCELoss(weigth)\n",
    "                        output = net(input)\n",
    "                        loss = criterion(output, target)\n",
    "                        test_losses.append(loss.data)\n",
    "                        print('test: ' + str(len(test_losses)) + '/' + str(len(test_dataloader)) + '          ', end = '\\r')\n",
    "                    test_loss = np.sum(test_losses) / len(test_losses)\n",
    "\n",
    "            live_graph.step(train_loss, validation_loss, test_loss)\n",
    "\n",
    "            if validation_loss <= validation_threshold:\n",
    "                print('Done training.')\n",
    "                break\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import kaldi_utils\n",
    "\n",
    "def tracking_tester(recordings_segments,\n",
    "                    recordings_ids = None,\n",
    "                    scoring_function = None,\n",
    "                    groundtruth_filepath = '',\n",
    "                    groundtruth_valid_speakers_ids = ['A', 'B'],\n",
    "                    vector = 'ivectors',\n",
    "                    models_container_length = 2,\n",
    "                    models_container_include_overlaps = False,\n",
    "                    models_generation_length = 3,\n",
    "                    models_generation_selection = 'first',\n",
    "                    save_dir = 'tmp'):\n",
    "\n",
    "    if recordings_ids is None:\n",
    "        recordings_ids = [recording_id for recording_id in recordings_segments]\n",
    "    recordings_ids = recordings_ids if isinstance(recordings_ids, list) else [recordings_ids]\n",
    "    recordings_ids.sort()\n",
    "    \n",
    "    results = {}\n",
    "    results_reduced = {}\n",
    "    results_rttm = ''\n",
    "    results_scores = {}\n",
    "    eer_scores = ''\n",
    "    dcf_scores = ''\n",
    "    dcf_trials = ''\n",
    "    for i, recording_id in enumerate(recordings_ids):\n",
    "        print('tracking running: recording ' + str(i + 1) + '/' + str(len(recordings_ids)), end = '\\r')\n",
    "        # ----- Generating the models of each speaker in this recording ----- #\n",
    "        recording_dataset = Recordings_dataset(recordings_segments,\n",
    "                                               recording_id,\n",
    "                                               vector = vector,\n",
    "                                               models_container_length = models_container_length,\n",
    "                                               models_container_include_zeros = False,\n",
    "                                               models_container_include_overlaps = models_container_include_overlaps,\n",
    "                                               models_generation_lengths = [models_generation_length],\n",
    "                                               models_generation_selection = models_generation_selection)\n",
    "        speakers_models = recording_dataset.recordings_data[recording_id]['speakers_models']\n",
    "        speakers_segments_indexes_lengths_max = recording_dataset.recordings_data[recording_id]['speakers_segments_indexes_lengths_max']\n",
    "        speakers_ids = [speakers_ids for speakers_ids in speakers_models]\n",
    "        models_container = [speakers_models[speakers_ids][models_generation_length][0] for speakers_ids in speakers_models if models_container_include_overlaps or len(speakers_ids.split(',')) == 1]\n",
    "        # ----- Filling with 0's the remaining spaces of the speaker's models container ----- #\n",
    "        for i in range(models_container_length - len(models_container)):\n",
    "            models_container.append(np.random.uniform(-0.1, 0.1, len(models_container[0])))\n",
    "            \n",
    "        # ----- Obtaining the recording segments indexes ----- #\n",
    "        recording_segments_indexes = [(recording_id, index, True) for index, segment, in enumerate(recordings_segments[recording_id])]\n",
    "        \n",
    "        '''# ----- Generating the false segments indexes ----- #\n",
    "        other_recordings_segments = {}\n",
    "        other_recordings_segments_indexes = []\n",
    "        \n",
    "        for other_recording_id in recordings_segments:\n",
    "            if other_recording_id != recording_id:\n",
    "                other_recordings_segments[other_recording_id] = []\n",
    "                #other_recordings_segments_indexes += [(other_recording_id, index, False) for index, segment in enumerate(recordings_segments[other_recording_id])]\n",
    "                other_dataset = Recordings_dataset(recordings_segments,\n",
    "                                                   other_recording_id,\n",
    "                                                   vector = vector,\n",
    "                                                   models_container_length = models_container_length,\n",
    "                                                   models_container_include_zeros = False,\n",
    "                                                   models_container_include_overlaps = models_container_include_overlaps,\n",
    "                                                   models_generation_lengths = [models_generation_length],\n",
    "                                                   models_generation_selection = models_generation_selection)\n",
    "                other_speakers_models = other_dataset.recordings_data[other_recording_id]['speakers_models']\n",
    "                other_speakers_segments = [other_dataset.recordings_data[other_recording_id]['speakers_segments_indexes'][speakers_ids][0] for speakers_ids in other_speakers_models]\n",
    "                other_speakers_segments = [copy.deepcopy(recordings_segments[segment_recording_id][segment_index]) for segment_recording_id, segment_index, segment_real in other_speakers_segments]\n",
    "                other_speakers_models = [other_speakers_models[speakers_ids][models_generation_length][0] for speakers_ids in other_speakers_models]\n",
    "                for index, segment in enumerate(other_speakers_segments):\n",
    "                    other_speakers_segments[index][vector][0]['value'] = other_speakers_models[index]\n",
    "                    other_recordings_segments[other_recording_id].append(other_speakers_segments[index])\n",
    "                    other_recordings_segments_indexes.append((other_recording_id, len(other_recordings_segments[other_recording_id]) - 1, False))                \n",
    "\n",
    "        #other_recordings_segments_indexes = random.sample(other_recordings_segments_indexes, speakers_segments_indexes_lengths_max if speakers_segments_indexes_lengths_max < len(other_recordings_segments_indexes) else len(other_recordings_segments_indexes))    \n",
    "        options = [recording_segments_indexes, other_recordings_segments_indexes]\n",
    "        options_lengths = [len(option) for option in options]\n",
    "        new_recording_segments_indexes = []\n",
    "        while sum(options_lengths) > 0:\n",
    "            options_indexes = list(itertools.chain(*[[index] * len(option) for index, option in enumerate(options)]))\n",
    "            option_index = random.choice(options_indexes)\n",
    "            new_recording_segments_indexes.append(options[option_index].pop(0))\n",
    "            options_lengths = [len(option) for option in options]\n",
    "        recording_segments_indexes = new_recording_segments_indexes'''\n",
    "        \n",
    "        # ----- Obtaining the recording tracking results ----- #\n",
    "        results[recording_id] = []\n",
    "        results_scores[recording_id] = []\n",
    "        for segment_recording_id, segment_index, segment_real in recording_segments_indexes:\n",
    "            #segment = recordings_segments[segment_recording_id][segment_index]\n",
    "            if segment_real:\n",
    "                segment = recordings_segments[segment_recording_id][segment_index]\n",
    "            else:\n",
    "                segment = other_recordings_segments[segment_recording_id][segment_index]\n",
    "            segment_vector = np.asarray(segment[vector][0]['value'])\n",
    "            segment_vector_id = segment[vector][0]['ivector_id' if vector == 'ivectors' else 'xvector_id']\n",
    "            \n",
    "            scores = scoring_function(segment_vector, models_container)\n",
    "            \n",
    "            targets_ids = sorted([speaker['speaker_id'] for speaker in segment['speakers']])\n",
    "            labels = ['target' if segment_real and targets_ids == sorted(speaker_id.split(',')) else 'nontarget' for speaker_id in speakers_ids]\n",
    "            \n",
    "            utterances = [recording_id + '_' + speaker_id for speaker_id in speakers_ids]\n",
    "            \n",
    "            # utt1, utt2, score, target/nontarget\n",
    "            scores_labels = list(zip([segment_vector_id for speaker_id in speakers_ids], utterances, labels, scores))\n",
    "            results_scores[recording_id].append(scores_labels)\n",
    "            \n",
    "            if segment_real:\n",
    "                index = np.argmax(scores)\n",
    "                results[recording_id].append({ 'begining': segment['begining'], 'ending': segment['ending'], 'speaker_id': index })\n",
    "                if len(results[recording_id]) > 2:\n",
    "                    if results[recording_id][len(results[recording_id]) - 1]['speaker_id'] == results[recording_id][len(results[recording_id]) - 3]['speaker_id']:\n",
    "                        if results[recording_id][len(results[recording_id]) - 1]['speaker_id'] != results[recording_id][len(results[recording_id]) - 2]['speaker_id']:\n",
    "                            results[recording_id][len(results[recording_id]) - 2]['speaker_id'] = results[recording_id][len(results[recording_id]) - 1]['speaker_id']\n",
    "                            results[recording_id][len(results[recording_id]) - 1]['modified'] = True\n",
    "        results_reduced[recording_id] = []\n",
    "        last_speaker_id = -1\n",
    "        last_speaker = { 'begining': 0, 'ending': 0, 'speaker_id': -1 }\n",
    "        for segment in results[recording_id] + [{ 'begining': 0, 'ending': 0, 'speaker_id': -1 }]:\n",
    "            begining = segment['begining']\n",
    "            ending = segment['ending']\n",
    "            speaker_id = segment['speaker_id']\n",
    "            if last_speaker_id != speaker_id:\n",
    "                if last_speaker_id != -1:\n",
    "                    results_reduced[recording_id].append(last_speaker)\n",
    "                last_speaker_id = speaker_id\n",
    "                last_speaker = { 'begining': begining, 'ending': ending, 'speaker_id': speaker_id }\n",
    "            else:\n",
    "                if begining <= last_speaker['ending']:\n",
    "                    last_speaker['ending'] = ending\n",
    "                else:\n",
    "                    if last_speaker_id != -1:\n",
    "                        results_reduced[recording_id].append(last_speaker)\n",
    "                    last_speaker_id = speaker_id\n",
    "                    last_speaker = { 'begining': begining, 'ending': ending, 'speaker_id': speaker_id }\n",
    "        for scores_labels in results_scores[recording_id]:\n",
    "            for score_label in scores_labels:\n",
    "                # ('iaab_000-00000000-00000099', 'iaab_B', 'target', 0.9978078)\n",
    "                eer_score = '{:f}'.format(score_label[3]) + ' ' + score_label[2]\n",
    "                eer_scores += eer_score + '\\n'\n",
    "                dcf_score = score_label[0] + ' ' + score_label[1] + ' ' + '{:f}'.format(score_label[3])\n",
    "                dcf_scores += dcf_score + '\\n'\n",
    "                dcf_trial = score_label[0] + ' ' + score_label[1] + ' '+ score_label[2]\n",
    "                dcf_trials += dcf_trial + '\\n'\n",
    "        for segment in results_reduced[recording_id]:\n",
    "            result_rttm = 'SPEAKER ' + recording_id + ' 1 ' + str(segment['begining']) + ' ' + str(round(segment['ending'] - segment['begining'], 2)) + ' <NA> <NA> ' + str(segment['speaker_id']) + ' <NA> <NA>'\n",
    "            results_rttm += result_rttm + '\\n'\n",
    "    print('traking done: recording', str(i + 1) + '/' + str(len(recordings_ids)), '          ')\n",
    "\n",
    "    file = open(groundtruth_filepath, 'r')\n",
    "    groundtruth_rttm = ''.join([line for line in file.readlines() if (line.split(' ')[1] in recordings_ids) and \\\n",
    "                    (line.split(' ')[7] in groundtruth_valid_speakers_ids)])\n",
    "    file.close()\n",
    "    \n",
    "    !mkdir -p $save_dir\n",
    "    \n",
    "    file = open(save_dir + '/eer.scores', 'w')\n",
    "    file.write(eer_scores)\n",
    "    file.close()\n",
    "    \n",
    "    file = open(save_dir + '/dcf.scores', 'w')\n",
    "    file.write(dcf_scores)\n",
    "    file.close()\n",
    "    \n",
    "    file = open(save_dir + '/dcf.trials', 'w')\n",
    "    file.write(dcf_trials)\n",
    "    file.close()\n",
    "    \n",
    "    file = open(save_dir + '/groundtruth.rttm', 'w')\n",
    "    file.write(groundtruth_rttm)\n",
    "    file.close()\n",
    "    \n",
    "    file = open(save_dir + '/results.rttm', 'w')\n",
    "    file.write(results_rttm)\n",
    "    file.close()\n",
    "    \n",
    "    output_der = kaldi_utils.md_eval(save_dir + '/groundtruth.rttm', save_dir + '/results.rttm', log_directory=save_dir)\n",
    "    output_eer = kaldi_utils.compute_eer(save_dir + '/eer.scores', log_directory=save_dir)\n",
    "    output_dcf = kaldi_utils.compute_min_dcf(save_dir + '/dcf.scores', save_dir + '/dcf.trials', log_directory=save_dir)\n",
    "\n",
    "    return { 'der': output_der, 'eer': output_eer, 'dcf': output_dcf }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading recordings segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_directory = '../exp/pre_norm/dihard_2019_dev/json'\n",
    "a_groundtruth = '../data/dihard_2019_dev_1.0_0.5.rttm'\n",
    "a_plda = '../exp/plda/dihard_2019_dev/ivectors.plda'\n",
    "b_directory = '../exp/pre_norm/dihard_2019_eval/json'\n",
    "b_groundtruth = '../data/dihard_2019_eval_1.0_0.5.rttm'\n",
    "b_plda = '../exp/plda/dihard_2019_eval/ivectors.plda'\n",
    "maximum_speakers_length = 1\n",
    "valid_speakers_ids = ['A', 'B']\n",
    "models_container_length = 2\n",
    "models_container_include_zeros = True\n",
    "models_container_include_overlaps = False\n",
    "models_generation_selection = 'first'\n",
    "balance_segments_selection = 'copy'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../exp/pre_norm/dihard_2019_dev/json loaded 192/192, 0.73 segments left.\n",
      "../exp/pre_norm/dihard_2019_eval/json loaded 194/194, 0.76 segments left.\n"
     ]
    }
   ],
   "source": [
    "a_recordings_segments = load_recordings_segments(a_directory,\n",
    "  lambda segment: is_valid_segment(segment, maximum_speakers_length, valid_speakers_ids))\n",
    "b_recordings_segments = load_recordings_segments(b_directory,\n",
    "  lambda segment: is_valid_segment(segment, maximum_speakers_length, valid_speakers_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../exp/pre_norm/dihard_2019_dev/json loaded 192/192, 0.73 segments left.\n",
      "../exp/pre_norm/dihard_2019_eval/json loaded 194/194, 0.76 segments left.\n"
     ]
    }
   ],
   "source": [
    "a_recordings_test_segments = load_recordings_segments(a_directory,\n",
    "  lambda segment: is_valid_segment(segment, maximum_speakers_length, valid_speakers_ids))\n",
    "b_recordings_test_segments = load_recordings_segments(b_directory,\n",
    "  lambda segment: is_valid_segment(segment, maximum_speakers_length, valid_speakers_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_selector(vector, models_container, net):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    with torch.no_grad():\n",
    "        input = [torch.Tensor([nparray]).to(device, non_blocking = True).float() for nparray in [vector] + models_container]\n",
    "        output = net(input)\n",
    "        return output.cpu().data.numpy()[0]\n",
    "    \n",
    "def plda_selector(vector, models_container, plda_filepath):\n",
    "    output = [kaldi_utils.ivector_plda_scoring(plda_filepath, ref_vector, vector) for ref_vector in models_container]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ivectors 128 5 0\n",
      "traking done: recording 192/192           \n",
      "traking done: recording 194/194           \n",
      "{'der': 18.0, 'eer': 34.76, 'dcf': 0.9991}\n",
      "{'der': 19.93, 'eer': 38.48, 'dcf': 0.9989}\n",
      "{'der': 18.965, 'eer': 36.62, 'dcf': 0.999}\n",
      "ivectors 128 10 0\n",
      "traking done: recording 192/192           \n",
      "traking done: recording 194/194           \n",
      "{'der': 15.85, 'eer': 32.93, 'dcf': 0.9994}\n",
      "{'der': 16.56, 'eer': 36.55, 'dcf': 0.9984}\n",
      "{'der': 16.205, 'eer': 34.739999999999995, 'dcf': 0.9988999999999999}\n",
      "ivectors 128 20 0\n",
      "traking done: recording 192/192           \n",
      "traking done: recording 194/194           \n",
      "{'der': 13.04, 'eer': 31.94, 'dcf': 0.9997}\n",
      "{'der': 13.45, 'eer': 35.46, 'dcf': 0.9991}\n",
      "{'der': 13.245, 'eer': 33.7, 'dcf': 0.9994000000000001}\n",
      "xvectors 128 5 0\n",
      "tracking running: recording 1/192\r"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "b\"../../../../src/ivectorbin/ivector-plda-scoring ../exp/plda/dihard_2019_eval/ivectors.plda 'ark:echo reference [ 0.29887644999999996 0.655712754 2.4484079999999997 0.11420546000000002 -2.6883632 -0.6488181000000001 1.1027851 0.27351765999999994 0.34784384 -0.63196858 0.9466455 -0.5511643100000001 0.36318316 -0.83090606 1.8994997999999998 0.13442614660000002 -0.75489266 0.204534672 0.587454934 1.4194091199999999 -0.36740632 0.8159236400000001 -0.7148888999999998 -1.37312248 -1.48805174 -1.17451924 -0.7411837 -1.95732028 0.32107281800000004 0.7284440240000001 0.23449213999999996 -0.587963188 1.3800577399999998 -0.34241637199999997 -0.41816140399999996 0.44427391399999994 -0.41278494000000004 -0.5296680119999999 -0.15180084000000002 1.4726454 0.09655533999999996 -0.77868604 0.71592164 0.03607330000000001 -0.4806382 1.118451846 -1.8498265800000002 0.62604682 -0.33401835599999996 -0.45854042 0.65091916 -1.8118020000000001 -1.42003896 0.7335996 -0.15220707 0.65367094 -0.11524976000000002 0.40669910600000003 -0.08477257999999999 1.37694818 -0.4980825400000001 0.43449599799999994 -0.5414729 -0.82794516 0.44319152000000006 -0.32153216 0.34234237199999995 1.06548732 -0.55591772 0.14288542199999998 0.570166576 -0.216797036 -0.47250922000000006 0.05769741999999996 0.19869404 0.16611832599999998 0.15777892000000002 -1.3665414999999999 -0.83516776 -0.16073584 0.11054508 -0.42566172 0.0436370646 0.39002016 0.40958522 0.4353897 0.1304615 0.33215459999999997 -0.32973748999999997 -0.6588309299999999 -0.670548234 -0.70855386 -0.9403508200000001 -0.15329144000000006 0.9072766939999999 0.8107907900000001 0.58215052 0.22472612600000003 0.79444812 -0.19696126200000003 0.25945444000000006 0.27361023999999995 0.13826754000000002 0.24813238 -0.48981226 -0.10085251999999996 -0.5049277919999999 -0.7290439599999999 1.4461411638 0.244771376 -0.6204486214 0.536370842 -0.46710938 0.09890724000000002 0.41827565799999994 1.26566184 0.12674566199999998 0.38684215999999993 0.21846986999999998 0.27406454 -0.41458510000000004 -1.54998858 0.3891795800000001 -0.2933902532 -0.9922314 0.11590252000000001 -0.8073775599999999 -1.092735504 0.788922688 0.448099656 -0.5332193199999999 -0.25411434 -0.333279144 0.5425552140000001 -0.11778755 0.094376508 -0.16352675999999994 0.039458082000000005 -0.43462181 0.65331886 1.1193954600000002 1.02584316 0.8035570400000001 0.92411022 -0.339115616 -0.04334285 1.2236716299999997 0.11134817999999998 -0.8890229399999999 0.0004356000000000027 0.59912146 -1.09369774 0.9583555539999999 -0.23000268799999998 -0.19833459999999997 -1.1079974 0.05215888 -0.5725793880000001 0.52323728 0.13153332 -0.47038028 -0.07712919999999998 0.7938381400000001 -0.54200213 -0.21633161999999997 0.4733522432 -1.04724886 0.6075347 -0.22237933999999998 1.1266147399999997 0.20516788200000002 -0.005989280000000008 1.166916604 0.707624356 -1.0366673 -0.5038883599999999 -0.5359507000000001 -0.38042748 0.55965636 0.13430457999999995 0.58673718 -0.14120672 -0.9626317140000001 0.27962338 -0.31850669479999993 1.18812888 -0.41417660199999995 -0.14960396999999998 0.74811382 -0.043924131999999984 0.37630938 -0.55151072 -0.09454461999999997 0.948557482 0.54476846 0.96748054 -0.4822778200000001 1.0852447 0.20323017999999998 0.046643625999999994 -0.17156783999999997 -0.022644719999999972 0.24485806400000004 -0.25373335999999996 -1.63202267 0.532166008 0.17737412999999996 0.38015733900000004 -0.47277962 1.08215302 -0.07412962799999999 0.8026580259999998 0.40908533999999996 0.34904605199999994 0.40296088 0.634741176 1.4562141199999998 0.74556428 -0.10728196000000001 -0.060095180000000095 0.14863288 -1.4044253 -0.7986968400000001 -0.5811145200000001 -0.53339768 0.50116922 -0.46316754 -0.6632619399999999 0.55712684 0.34419378 -0.140613416 0.41776403999999995 -0.47385487400000004 0.52023 0.7344096999999999 -1.37062218 -0.05760999699999998 0.08656766000000005 0.145342949 0.9769996800000001 1.3287115600000001 -0.413788176 -1.6588587000000001 -0.29283519999999996 0.06583642199999998 -0.28411110000000006 0.105329834 -0.89782238 -0.7960553599999999 0.06584715999999996 -0.43519134 0.752799082 -0.35422898 -0.166734742 0.642440248 -0.5650445 0.72130256 -0.50626678 -0.471012082 -0.44077800000000006 0.8821066200000001 -0.032060465999999996 -0.37359358 -0.9623975799999999 0.56533594 -1.0788571 0.26826425 -1.3658716 0.33579658000000007 -1.22624458 -0.9746404 -0.25999353 0.10197165400000001 0.989493216 -0.24188229 -0.284331196 0.23495088000000003 -0.29779476000000005 0.12553743999999994 0.09155582000000004 0.9827863239999999 0.9525776800000001 -0.033447038 -0.244975788 0.652751836 0.11641562000000003 -0.43964277999999996 -0.263054198 0.3108229 -0.18188957199999997 -0.18779182 -0.998460998 0.047957139999999995 0.6507386000000001 -0.65803408 -0.44983547599999996 1.01514452 -0.5321518 -0.6908951 -0.07327186000000001 -0.33732296840000003 -0.07222440399999996 -0.186828176 0.73204246 -0.018976840000000016 -0.36240023400000004 1.413519618 -0.186915014 0.38476721 -1.15958982 -0.5699304599999999 0.03966263600000001 0.6159103 0.27382734200000003 -0.8057242920000002 0.39449556 -0.7896978800000001 -0.9286289999999999 0.61411954 0.8256889399999998 -0.1642173 -0.18469055 -0.9099344 0.44402162 -0.38925954 0.49499726 -0.06208233999999998 0.26917829 -0.9304829139999999 -0.45432746 -0.14738299999999996 0.5416079889999998 0.736864542 0.80293291 -0.7679461799999999 -0.26428492 -1.50857136 -0.7323756600000001 -1.2662383000000001 0.12268026000000001 -0.15004678 -0.5805085320000001 0.11591963600000002 -0.7990742799999999 -0.6927988399999999 0.8690866000000002 0.9090505600000001 0.12515103 0.067988752 -0.5483210520000001 -0.18570247439999998 0.053255898 0.664608122 0.5801021000000001 -0.37982576 -0.6221490199999999 -0.7060550000000001 -0.9018084 0.7029759400000001 -0.03461137000000001 0.6442233847999999 1.18106974 0.07364653800000001 0.4990203796 0.42614169999999996 -0.7385424 0.24113989880000003 0.06666741999999998 0.36197004 0.245272808 0.36692638 -0.09266668000000008 -0.18056014 -0.13777456 0.27494516 -0.1512551 0.7785013399999999 -0.048170538 -0.6740261999999999 0.16500124 -0.83475316 -0.45371792000000005 0.20214452 -0.13781904 -0.5494967 -0.185702242 -0.12072137999999996 0.6230125400000001 0.47397133140000003 -0.28965458 0.38880204200000007 -0.46523124599999993 0.46225630000000006 0.22657523999999998 0.27907531599999996 -0.13505048 -0.74192674 0.6592866 0.18970120259999998 0.17768225399999998 0.30358005999999993 -0.42760439840000003 -0.31558286 -1.07905376 0.6392585000000001 0.47337808 1.4045079800000002 -0.31768389199999997 0.30602717999999995 0.39386846 -0.9925884999999999 -0.49714912 -0.20005788 0.46886175999999996 -0.58923684 0.06569366000000001 -0.21188690799999993 0.48616934 -0.43549812 -0.7942428400000001 -0.021490812000000005 0.4540197 1.3054430200000002 0.006953357999999986 0.6998655 -0.6858027019999999 0.12430129999999999 -0.9634988400000001 -1.0922337160000002 -1.2444253800000002 0.27290321999999995 -0.22420682000000003 -0.32618553399999994 -0.02144904000000001 -0.58314278 0.04675156000000002 0.1780089946 -0.552107044 0.5945399 0.47152582 -0.9933838399999999 -0.40733578000000004 -0.15294606 -0.329137642 -0.31336662000000004 -0.6115972299999999 0.16356505999999998 -0.7411073000000001 0.142521886 0.59375972 0.37891753999999994 -0.08205914000000003 -0.324905854 -0.23590764000000003 0.816022698 -0.40369666000000004 -0.6523536999999999 -0.8825432599999999 -0.16200572 -0.5983994100000001 -0.253472804 -0.9371075400000001 0.09134151999999998 0.33027544 -0.20009214320000002 -0.18724810000000006 0.16959915800000003 -0.72095259 0.007295117999999956 -0.72808733578 0.591431592 -1.0210456399999999 -1.1342494760000001 0.19890604000000006 -0.48222655999999997 0.32700135999999996 0.321353028 0.58415807 -1.00687292 -0.33264726 0.15625313 0.20795735999999998 0.09221354000000001 -0.44406806000000004 0.66418576 -1.6294286 0.69029692 0.7816171998 1.13337076 -0.611994194 0.43954950000000004 -0.6416196999999999 1.06336268 0.10398787999999999 -0.46337747999999995 0.59063878 -0.12298176000000001 0.58756476 0.031022969999999983 -0.040090152000000004 -0.3258711148 -0.5048208200000001 -0.010658859999999982 0.10988546000000006 0.24415608400000002 -0.11622762 0.954902744 -0.2600591440000001 0.10167486959999998 -0.059620519999999996 0.476331626 -1.02836218 ]|' 'ark:echo test [ -0.04333701 1.29962 1.94257 -1.210734 -2.444127 -0.9736633 0.2200095 -0.182941 0.9895858 -0.5028466 1.000439 1.207608 0.1131659 -0.8649002 1.10268 -1.040714 -1.688753 0.9290887 0.8307573 3.295411 0.839706 -0.0513294 -1.134488 -1.152909 -1.34266 -1.607064 0.4482984 -2.944324 0.3603738 -0.9169869 0.8957786 0.5587631 0.8699567 -0.3208606 -1.680744 -0.04968333 -0.1333144 0.1184384 -0.4989972 0.518005 -0.4396986 -0.858314 1.16537 -1.443277 0.7151956 1.970478 -0.7440999 0.4118074 0.1290299 -0.2192283 0.6649606 -1.632289 -0.1330336 0.834544 0.827698 -0.3518302 0.6204565 0.3272862 -0.7162793 1.510789 -0.573659 -0.3949648 0.7883891 -0.1018859 1.611057 -0.6067246 -0.1436379 0.1174736 0.3317728 0.8798273 2.326488 0.9312856 1.517806 0.3079171 0.5698612 0.03622273 1.132539 -0.1423428 -0.9193982 1.733457 -0.1974743 0.9090331 0.06198817 0.4539106 0.6820314 -1.213899 0.8565007 2.497707 -0.4289833 -0.1331706 -0.6090173 -1.197819 -0.563417 0.5432696 2.544293 -0.1568151 2.285014 0.2346221 2.041039 0.7633252 0.7844767 0.5923362 0.2858239 -1.611942 -0.3425087 -0.3608235 -0.7318746 -1.427572 2.084067 -0.8253872 -0.8166701 -0.9204171 -0.5391461 1.53631 0.4604161 0.9799795 -0.2116867 1.788832 1.022215 0.9966913 1.024943 -2.437841 0.5924428 0.634388 -1.701217 0.3784846 -0.8851932 0.06365738 -0.08586806 0.2539998 -0.7739933 0.2151243 -0.3745156 -0.01660663 -0.6388945 -0.5303732 -0.2402205 0.01800291 0.3991498 1.312578 0.2148615 1.661915 0.5056676 2.002777 -0.6451952 0.9418684 1.918502 -0.9493734 -0.5433381 -0.6537203 0.9525415 -2.074615 0.5683705 0.6513057 1.103118 -1.011237 1.008656 -0.9885293 0.181264 -0.5189127 -1.721403 -1.942393 -1.658586 -0.01262385 -0.2051173 0.2439763 -1.636589 0.4620348 -0.958582 0.4577186 1.162067 0.3769418 2.108469 2.084106 -2.941989 -0.1295029 -0.3806217 -0.6768492 1.271883 -0.3393054 -0.2139415 0.7782294 -0.09982467 -0.8774156 -0.3959666 0.5673009 -0.7962337 -0.1302282 0.3039638 -0.4760045 2.221546 -0.6320672 -0.1734713 0.05667791 0.2851898 0.3488475 -1.810459 1.256748 -0.2082566 1.224836 0.1156131 1.205904 0.6032956 0.6425929 -2.562021 -0.04039646 -0.3360477 0.004428295 0.5886461 1.747827 0.5912499 1.55805 0.4088729 0.9822685 1.837816 0.8338922 1.916281 0.599391 -0.8759986 -1.931186 -1.504072 -1.229609 -0.3005431 0.2705378 1.636639 1.970807 0.119444 -0.4108988 0.6244572 -0.1402336 0.01818202 1.960536 -0.07934393 -0.139521 1.238755 -0.3907099 0.3963101 1.904699 -0.4175698 0.6008816 1.245439 -0.07717209 -2.281146 -1.336947 1.403016 -0.7024497 0.04852669 -1.470156 -1.574422 -1.172509 -0.1526515 0.5355746 1.167698 -0.1179011 0.01671907 -0.5637903 0.399344 0.4229684 0.2095394 0.3006929 -0.7405396 -0.8293047 -0.7948641 0.4510973 0.48273 -0.938422 0.1333091 -1.049161 -0.4189985 -0.8688979 -2.356058 -0.3459733 0.05610537 0.2599796 -0.5900297 -0.4372243 0.1378773 -0.5442951 -0.3552969 1.851148 0.8002893 2.056789 -0.02279629 0.09894016 -1.293905 0.7102395 -0.8731477 -0.09681189 0.1296617 -1.259479 0.5376722 -1.098291 0.622221 0.7707724 -0.4183445 0.02821237 0.2722122 -1.229902 -1.663839 0.6309426 0.6806481 -1.712393 0.07076532 0.1622998 1.40793 -0.3268313 0.04690409 1.45798 0.4481483 0.8001958 -0.793367 -0.08323065 -0.279645 -0.2280513 -0.8918971 -0.742931 -0.6825243 -0.8161095 0.1671209 0.3450058 -1.329386 0.277076 1.040974 0.8816969 -0.742685 0.1682459 -0.7402844 0.1444277 -1.685473 -0.7175495 -0.9909665 0.5337318 -0.3928823 0.1472097 0.2929369 -1.075492 -3.069526 0.660839 -1.082643 -0.9046736 0.2777066 0.3003284 0.2398861 -0.8961425 -0.6810062 1.388365 0.5795922 0.09299275 -0.00519904 0.09758264 -0.003806772 -0.1129294 0.4702265 1.476398 0.8871791 0.5537923 -1.415165 -0.963003 1.542403 -0.09986245 0.7980935 2.129976 -1.36261 0.2934496 -0.2021564 0.3290994 -0.9636427 0.4761668 0.2098201 1.139049 0.5374911 1.803337 -1.797423 -0.1760459 -0.5898446 -1.274134 0.7996708 0.01400542 -1.468084 0.7672077 -0.9410215 -0.6170987 -1.464188 0.3579551 0.2872465 0.2722376 -0.3373924 0.6828305 1.343636 1.124016 0.1186896 -0.7105294 -0.6300582 0.5813752 -0.8007836 0.1144793 0.6949525 1.801873 0.165754 0.1886586 0.4404994 0.6414157 -0.5423913 -0.80247 0.2314336 1.128113 1.780874 0.5831343 -1.624182 1.11799 -0.3005862 -0.7040543 -0.1317759 0.3682976 -0.886638 -0.6724426 -1.080427 0.6306146 -0.1455592 -0.7161114 -0.01017866 0.3793837 1.575801 0.2083986 -0.1038426 -0.9342206 0.9610823 -0.1874037 -0.02123098 -0.9144708 -0.6838039 -1.279805 0.5671941 -0.1966533 -1.016213 -0.1513035 0.001416673 -2.040415 1.759163 -0.4645828 0.8261443 0.418042 1.267045 -0.7910001 1.918154 -1.972973 0.1807459 -0.3720856 1.857852 0.9988148 -1.581336 -1.119879 -0.1947116 -0.1095404 0.420513 -0.7855236 0.7851456 -1.572063 -0.6097915 0.3689607 0.4990093 -0.8292907 -0.5944802 0.1878158 -0.004948216 -1.106262 -0.8588536 -0.8120697 0.8607093 0.0003040211 1.171101 0.0415291 -0.7584587 0.6338635 -0.590177 -0.175535 0.7635918 0.04384365 -0.1514411 0.4210753 -0.1853794 -1.701955 0.7474419 -0.9298939 -1.732318 -0.975243 -0.1865974 1.348904 0.7414991 0.3510095 1.024958 -1.878345 -0.4480546 1.347763 0.92971 0.3551055 -1.340726 0.3723226 0.07473015 -0.4415409 -0.3007682 -1.071372 0.4303098 0.8044738 0.3725953 -0.5678942 1.383337 -0.5912866 -1.517736 0.4459729 0.575776 -1.42056 ]|' 'echo reference test|' '|cat' \\nLOG (ivector-plda-scoring[5.5.0~1-e5a5]:main():ivector-plda-scoring.cc:96) Reading train iVectors\\nASSERTION_FAILED (ivector-plda-scoring[5.5.0~1-e5a5]:TransformIvector():plda.cc:124) Assertion failed: (ivector.Dim() == Dim() && transformed_ivector->Dim() == Dim())\\n\\n[ Stack-Trace: ]\\n/opt/kaldi/src/lib/libkaldi-base.so(kaldi::MessageLogger::LogMessage() const+0x82c) [0x7fef0baa12aa]\\n/opt/kaldi/src/lib/libkaldi-base.so(kaldi::KaldiAssertFailure_(char const*, char const*, int, char const*)+0x6c) [0x7fef0baa1d18]\\n/opt/kaldi/src/lib/libkaldi-ivector.so(kaldi::Plda::TransformIvector(kaldi::PldaConfig const&, kaldi::VectorBase<double> const&, int, kaldi::VectorBase<double>*) const+0x48) [0x7fef0c19f072]\\n/opt/kaldi/src/lib/libkaldi-ivector.so(kaldi::Plda::TransformIvector(kaldi::PldaConfig const&, kaldi::VectorBase<float> const&, int, kaldi::VectorBase<float>*) const+0x99) [0x7fef0c19f1c1]\\n../../../../src/ivectorbin/ivector-plda-scoring(main+0x67f) [0x40be15]\\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0) [0x7fef0ac33830]\\n../../../../src/ivectorbin/ivector-plda-scoring(_start+0x29) [0x40b6c9]\\n\\n\"",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m b\"../../../../src/ivectorbin/ivector-plda-scoring ../exp/plda/dihard_2019_eval/ivectors.plda 'ark:echo reference [ 0.29887644999999996 0.655712754 2.4484079999999997 0.11420546000000002 -2.6883632 -0.6488181000000001 1.1027851 0.27351765999999994 0.34784384 -0.63196858 0.9466455 -0.5511643100000001 0.36318316 -0.83090606 1.8994997999999998 0.13442614660000002 -0.75489266 0.204534672 0.587454934 1.4194091199999999 -0.36740632 0.8159236400000001 -0.7148888999999998 -1.37312248 -1.48805174 -1.17451924 -0.7411837 -1.95732028 0.32107281800000004 0.7284440240000001 0.23449213999999996 -0.587963188 1.3800577399999998 -0.34241637199999997 -0.41816140399999996 0.44427391399999994 -0.41278494000000004 -0.5296680119999999 -0.15180084000000002 1.4726454 0.09655533999999996 -0.77868604 0.71592164 0.03607330000000001 -0.4806382 1.118451846 -1.8498265800000002 0.62604682 -0.33401835599999996 -0.45854042 0.65091916 -1.8118020000000001 -1.42003896 0.7335996 -0.15220707 0.65367094 -0.11524976000000002 0.40669910600000003 -0.08477257999999999 1.37694818 -0.4980825400000001 0.43449599799999994 -0.5414729 -0.82794516 0.44319152000000006 -0.32153216 0.34234237199999995 1.06548732 -0.55591772 0.14288542199999998 0.570166576 -0.216797036 -0.47250922000000006 0.05769741999999996 0.19869404 0.16611832599999998 0.15777892000000002 -1.3665414999999999 -0.83516776 -0.16073584 0.11054508 -0.42566172 0.0436370646 0.39002016 0.40958522 0.4353897 0.1304615 0.33215459999999997 -0.32973748999999997 -0.6588309299999999 -0.670548234 -0.70855386 -0.9403508200000001 -0.15329144000000006 0.9072766939999999 0.8107907900000001 0.58215052 0.22472612600000003 0.79444812 -0.19696126200000003 0.25945444000000006 0.27361023999999995 0.13826754000000002 0.24813238 -0.48981226 -0.10085251999999996 -0.5049277919999999 -0.7290439599999999 1.4461411638 0.244771376 -0.6204486214 0.536370842 -0.46710938 0.09890724000000002 0.41827565799999994 1.26566184 0.12674566199999998 0.38684215999999993 0.21846986999999998 0.27406454 -0.41458510000000004 -1.54998858 0.3891795800000001 -0.2933902532 -0.9922314 0.11590252000000001 -0.8073775599999999 -1.092735504 0.788922688 0.448099656 -0.5332193199999999 -0.25411434 -0.333279144 0.5425552140000001 -0.11778755 0.094376508 -0.16352675999999994 0.039458082000000005 -0.43462181 0.65331886 1.1193954600000002 1.02584316 0.8035570400000001 0.92411022 -0.339115616 -0.04334285 1.2236716299999997 0.11134817999999998 -0.8890229399999999 0.0004356000000000027 0.59912146 -1.09369774 0.9583555539999999 -0.23000268799999998 -0.19833459999999997 -1.1079974 0.05215888 -0.5725793880000001 0.52323728 0.13153332 -0.47038028 -0.07712919999999998 0.7938381400000001 -0.54200213 -0.21633161999999997 0.4733522432 -1.04724886 0.6075347 -0.22237933999999998 1.1266147399999997 0.20516788200000002 -0.005989280000000008 1.166916604 0.707624356 -1.0366673 -0.5038883599999999 -0.5359507000000001 -0.38042748 0.55965636 0.13430457999999995 0.58673718 -0.14120672 -0.9626317140000001 0.27962338 -0.31850669479999993 1.18812888 -0.41417660199999995 -0.14960396999999998 0.74811382 -0.043924131999999984 0.37630938 -0.55151072 -0.09454461999999997 0.948557482 0.54476846 0.96748054 -0.4822778200000001 1.0852447 0.20323017999999998 0.046643625999999994 -0.17156783999999997 -0.022644719999999972 0.24485806400000004 -0.25373335999999996 -1.63202267 0.532166008 0.17737412999999996 0.38015733900000004 -0.47277962 1.08215302 -0.07412962799999999 0.8026580259999998 0.40908533999999996 0.34904605199999994 0.40296088 0.634741176 1.4562141199999998 0.74556428 -0.10728196000000001 -0.060095180000000095 0.14863288 -1.4044253 -0.7986968400000001 -0.5811145200000001 -0.53339768 0.50116922 -0.46316754 -0.6632619399999999 0.55712684 0.34419378 -0.140613416 0.41776403999999995 -0.47385487400000004 0.52023 0.7344096999999999 -1.37062218 -0.05760999699999998 0.08656766000000005 0.145342949 0.9769996800000001 1.3287115600000001 -0.413788176 -1.6588587000000001 -0.29283519999999996 0.06583642199999998 -0.28411110000000006 0.105329834 -0.89782238 -0.7960553599999999 0.06584715999999996 -0.43519134 0.752799082 -0.35422898 -0.166734742 0.642440248 -0.5650445 0.72130256 -0.50626678 -0.471012082 -0.44077800000000006 0.8821066200000001 -0.032060465999999996 -0.37359358 -0.9623975799999999 0.56533594 -1.0788571 0.26826425 -1.3658716 0.33579658000000007 -1.22624458 -0.9746404 -0.25999353 0.10197165400000001 0.989493216 -0.24188229 -0.284331196 0.23495088000000003 -0.29779476000000005 0.12553743999999994 0.09155582000000004 0.9827863239999999 0.9525776800000001 -0.033447038 -0.244975788 0.652751836 0.11641562000000003 -0.43964277999999996 -0.263054198 0.3108229 -0.18188957199999997 -0.18779182 -0.998460998 0.047957139999999995 0.6507386000000001 -0.65803408 -0.44983547599999996 1.01514452 -0.5321518 -0.6908951 -0.07327186000000001 -0.33732296840000003 -0.07222440399999996 -0.186828176 0.73204246 -0.018976840000000016 -0.36240023400000004 1.413519618 -0.186915014 0.38476721 -1.15958982 -0.5699304599999999 0.03966263600000001 0.6159103 0.27382734200000003 -0.8057242920000002 0.39449556 -0.7896978800000001 -0.9286289999999999 0.61411954 0.8256889399999998 -0.1642173 -0.18469055 -0.9099344 0.44402162 -0.38925954 0.49499726 -0.06208233999999998 0.26917829 -0.9304829139999999 -0.45432746 -0.14738299999999996 0.5416079889999998 0.736864542 0.80293291 -0.7679461799999999 -0.26428492 -1.50857136 -0.7323756600000001 -1.2662383000000001 0.12268026000000001 -0.15004678 -0.5805085320000001 0.11591963600000002 -0.7990742799999999 -0.6927988399999999 0.8690866000000002 0.9090505600000001 0.12515103 0.067988752 -0.5483210520000001 -0.18570247439999998 0.053255898 0.664608122 0.5801021000000001 -0.37982576 -0.6221490199999999 -0.7060550000000001 -0.9018084 0.7029759400000001 -0.03461137000000001 0.6442233847999999 1.18106974 0.07364653800000001 0.4990203796 0.42614169999999996 -0.7385424 0.24113989880000003 0.06666741999999998 0.36197004 0.245272808 0.36692638 -0.09266668000000008 -0.18056014 -0.13777456 0.27494516 -0.1512551 0.7785013399999999 -0.048170538 -0.6740261999999999 0.16500124 -0.83475316 -0.45371792000000005 0.20214452 -0.13781904 -0.5494967 -0.185702242 -0.12072137999999996 0.6230125400000001 0.47397133140000003 -0.28965458 0.38880204200000007 -0.46523124599999993 0.46225630000000006 0.22657523999999998 0.27907531599999996 -0.13505048 -0.74192674 0.6592866 0.18970120259999998 0.17768225399999998 0.30358005999999993 -0.42760439840000003 -0.31558286 -1.07905376 0.6392585000000001 0.47337808 1.4045079800000002 -0.31768389199999997 0.30602717999999995 0.39386846 -0.9925884999999999 -0.49714912 -0.20005788 0.46886175999999996 -0.58923684 0.06569366000000001 -0.21188690799999993 0.48616934 -0.43549812 -0.7942428400000001 -0.021490812000000005 0.4540197 1.3054430200000002 0.006953357999999986 0.6998655 -0.6858027019999999 0.12430129999999999 -0.9634988400000001 -1.0922337160000002 -1.2444253800000002 0.27290321999999995 -0.22420682000000003 -0.32618553399999994 -0.02144904000000001 -0.58314278 0.04675156000000002 0.1780089946 -0.552107044 0.5945399 0.47152582 -0.9933838399999999 -0.40733578000000004 -0.15294606 -0.329137642 -0.31336662000000004 -0.6115972299999999 0.16356505999999998 -0.7411073000000001 0.142521886 0.59375972 0.37891753999999994 -0.08205914000000003 -0.324905854 -0.23590764000000003 0.816022698 -0.40369666000000004 -0.6523536999999999 -0.8825432599999999 -0.16200572 -0.5983994100000001 -0.253472804 -0.9371075400000001 0.09134151999999998 0.33027544 -0.20009214320000002 -0.18724810000000006 0.16959915800000003 -0.72095259 0.007295117999999956 -0.72808733578 0.591431592 -1.0210456399999999 -1.1342494760000001 0.19890604000000006 -0.48222655999999997 0.32700135999999996 0.321353028 0.58415807 -1.00687292 -0.33264726 0.15625313 0.20795735999999998 0.09221354000000001 -0.44406806000000004 0.66418576 -1.6294286 0.69029692 0.7816171998 1.13337076 -0.611994194 0.43954950000000004 -0.6416196999999999 1.06336268 0.10398787999999999 -0.46337747999999995 0.59063878 -0.12298176000000001 0.58756476 0.031022969999999983 -0.040090152000000004 -0.3258711148 -0.5048208200000001 -0.010658859999999982 0.10988546000000006 0.24415608400000002 -0.11622762 0.954902744 -0.2600591440000001 0.10167486959999998 -0.059620519999999996 0.476331626 -1.02836218 ]|' 'ark:echo test [ -0.04333701 1.29962 1.94257 -1.210734 -2.444127 -0.9736633 0.2200095 -0.182941 0.9895858 -0.5028466 1.000439 1.207608 0.1131659 -0.8649002 1.10268 -1.040714 -1.688753 0.9290887 0.8307573 3.295411 0.839706 -0.0513294 -1.134488 -1.152909 -1.34266 -1.607064 0.4482984 -2.944324 0.3603738 -0.9169869 0.8957786 0.5587631 0.8699567 -0.3208606 -1.680744 -0.04968333 -0.1333144 0.1184384 -0.4989972 0.518005 -0.4396986 -0.858314 1.16537 -1.443277 0.7151956 1.970478 -0.7440999 0.4118074 0.1290299 -0.2192283 0.6649606 -1.632289 -0.1330336 0.834544 0.827698 -0.3518302 0.6204565 0.3272862 -0.7162793 1.510789 -0.573659 -0.3949648 0.7883891 -0.1018859 1.611057 -0.6067246 -0.1436379 0.1174736 0.3317728 0.8798273 2.326488 0.9312856 1.517806 0.3079171 0.5698612 0.03622273 1.132539 -0.1423428 -0.9193982 1.733457 -0.1974743 0.9090331 0.06198817 0.4539106 0.6820314 -1.213899 0.8565007 2.497707 -0.4289833 -0.1331706 -0.6090173 -1.197819 -0.563417 0.5432696 2.544293 -0.1568151 2.285014 0.2346221 2.041039 0.7633252 0.7844767 0.5923362 0.2858239 -1.611942 -0.3425087 -0.3608235 -0.7318746 -1.427572 2.084067 -0.8253872 -0.8166701 -0.9204171 -0.5391461 1.53631 0.4604161 0.9799795 -0.2116867 1.788832 1.022215 0.9966913 1.024943 -2.437841 0.5924428 0.634388 -1.701217 0.3784846 -0.8851932 0.06365738 -0.08586806 0.2539998 -0.7739933 0.2151243 -0.3745156 -0.01660663 -0.6388945 -0.5303732 -0.2402205 0.01800291 0.3991498 1.312578 0.2148615 1.661915 0.5056676 2.002777 -0.6451952 0.9418684 1.918502 -0.9493734 -0.5433381 -0.6537203 0.9525415 -2.074615 0.5683705 0.6513057 1.103118 -1.011237 1.008656 -0.9885293 0.181264 -0.5189127 -1.721403 -1.942393 -1.658586 -0.01262385 -0.2051173 0.2439763 -1.636589 0.4620348 -0.958582 0.4577186 1.162067 0.3769418 2.108469 2.084106 -2.941989 -0.1295029 -0.3806217 -0.6768492 1.271883 -0.3393054 -0.2139415 0.7782294 -0.09982467 -0.8774156 -0.3959666 0.5673009 -0.7962337 -0.1302282 0.3039638 -0.4760045 2.221546 -0.6320672 -0.1734713 0.05667791 0.2851898 0.3488475 -1.810459 1.256748 -0.2082566 1.224836 0.1156131 1.205904 0.6032956 0.6425929 -2.562021 -0.04039646 -0.3360477 0.004428295 0.5886461 1.747827 0.5912499 1.55805 0.4088729 0.9822685 1.837816 0.8338922 1.916281 0.599391 -0.8759986 -1.931186 -1.504072 -1.229609 -0.3005431 0.2705378 1.636639 1.970807 0.119444 -0.4108988 0.6244572 -0.1402336 0.01818202 1.960536 -0.07934393 -0.139521 1.238755 -0.3907099 0.3963101 1.904699 -0.4175698 0.6008816 1.245439 -0.07717209 -2.281146 -1.336947 1.403016 -0.7024497 0.04852669 -1.470156 -1.574422 -1.172509 -0.1526515 0.5355746 1.167698 -0.1179011 0.01671907 -0.5637903 0.399344 0.4229684 0.2095394 0.3006929 -0.7405396 -0.8293047 -0.7948641 0.4510973 0.48273 -0.938422 0.1333091 -1.049161 -0.4189985 -0.8688979 -2.356058 -0.3459733 0.05610537 0.2599796 -0.5900297 -0.4372243 0.1378773 -0.5442951 -0.3552969 1.851148 0.8002893 2.056789 -0.02279629 0.09894016 -1.293905 0.7102395 -0.8731477 -0.09681189 0.1296617 -1.259479 0.5376722 -1.098291 0.622221 0.7707724 -0.4183445 0.02821237 0.2722122 -1.229902 -1.663839 0.6309426 0.6806481 -1.712393 0.07076532 0.1622998 1.40793 -0.3268313 0.04690409 1.45798 0.4481483 0.8001958 -0.793367 -0.08323065 -0.279645 -0.2280513 -0.8918971 -0.742931 -0.6825243 -0.8161095 0.1671209 0.3450058 -1.329386 0.277076 1.040974 0.8816969 -0.742685 0.1682459 -0.7402844 0.1444277 -1.685473 -0.7175495 -0.9909665 0.5337318 -0.3928823 0.1472097 0.2929369 -1.075492 -3.069526 0.660839 -1.082643 -0.9046736 0.2777066 0.3003284 0.2398861 -0.8961425 -0.6810062 1.388365 0.5795922 0.09299275 -0.00519904 0.09758264 -0.003806772 -0.1129294 0.4702265 1.476398 0.8871791 0.5537923 -1.415165 -0.963003 1.542403 -0.09986245 0.7980935 2.129976 -1.36261 0.2934496 -0.2021564 0.3290994 -0.9636427 0.4761668 0.2098201 1.139049 0.5374911 1.803337 -1.797423 -0.1760459 -0.5898446 -1.274134 0.7996708 0.01400542 -1.468084 0.7672077 -0.9410215 -0.6170987 -1.464188 0.3579551 0.2872465 0.2722376 -0.3373924 0.6828305 1.343636 1.124016 0.1186896 -0.7105294 -0.6300582 0.5813752 -0.8007836 0.1144793 0.6949525 1.801873 0.165754 0.1886586 0.4404994 0.6414157 -0.5423913 -0.80247 0.2314336 1.128113 1.780874 0.5831343 -1.624182 1.11799 -0.3005862 -0.7040543 -0.1317759 0.3682976 -0.886638 -0.6724426 -1.080427 0.6306146 -0.1455592 -0.7161114 -0.01017866 0.3793837 1.575801 0.2083986 -0.1038426 -0.9342206 0.9610823 -0.1874037 -0.02123098 -0.9144708 -0.6838039 -1.279805 0.5671941 -0.1966533 -1.016213 -0.1513035 0.001416673 -2.040415 1.759163 -0.4645828 0.8261443 0.418042 1.267045 -0.7910001 1.918154 -1.972973 0.1807459 -0.3720856 1.857852 0.9988148 -1.581336 -1.119879 -0.1947116 -0.1095404 0.420513 -0.7855236 0.7851456 -1.572063 -0.6097915 0.3689607 0.4990093 -0.8292907 -0.5944802 0.1878158 -0.004948216 -1.106262 -0.8588536 -0.8120697 0.8607093 0.0003040211 1.171101 0.0415291 -0.7584587 0.6338635 -0.590177 -0.175535 0.7635918 0.04384365 -0.1514411 0.4210753 -0.1853794 -1.701955 0.7474419 -0.9298939 -1.732318 -0.975243 -0.1865974 1.348904 0.7414991 0.3510095 1.024958 -1.878345 -0.4480546 1.347763 0.92971 0.3551055 -1.340726 0.3723226 0.07473015 -0.4415409 -0.3007682 -1.071372 0.4303098 0.8044738 0.3725953 -0.5678942 1.383337 -0.5912866 -1.517736 0.4459729 0.575776 -1.42056 ]|' 'echo reference test|' '|cat' \\nLOG (ivector-plda-scoring[5.5.0~1-e5a5]:main():ivector-plda-scoring.cc:96) Reading train iVectors\\nASSERTION_FAILED (ivector-plda-scoring[5.5.0~1-e5a5]:TransformIvector():plda.cc:124) Assertion failed: (ivector.Dim() == Dim() && transformed_ivector->Dim() == Dim())\\n\\n[ Stack-Trace: ]\\n/opt/kaldi/src/lib/libkaldi-base.so(kaldi::MessageLogger::LogMessage() const+0x82c) [0x7fef0baa12aa]\\n/opt/kaldi/src/lib/libkaldi-base.so(kaldi::KaldiAssertFailure_(char const*, char const*, int, char const*)+0x6c) [0x7fef0baa1d18]\\n/opt/kaldi/src/lib/libkaldi-ivector.so(kaldi::Plda::TransformIvector(kaldi::PldaConfig const&, kaldi::VectorBase<double> const&, int, kaldi::VectorBase<double>*) const+0x48) [0x7fef0c19f072]\\n/opt/kaldi/src/lib/libkaldi-ivector.so(kaldi::Plda::TransformIvector(kaldi::PldaConfig const&, kaldi::VectorBase<float> const&, int, kaldi::VectorBase<float>*) const+0x99) [0x7fef0c19f1c1]\\n../../../../src/ivectorbin/ivector-plda-scoring(main+0x67f) [0x40be15]\\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0) [0x7fef0ac33830]\\n../../../../src/ivectorbin/ivector-plda-scoring(_start+0x29) [0x40b6c9]\\n\\n\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "for vector in ['ivectors', 'xvectors']:\n",
    "    vector_length = 128 if vector == 'ivectors' else 128\n",
    "    for models_generation_length in [5, 10, 20]:\n",
    "        for i in range(1):\n",
    "            print(vector, vector_length, models_generation_length, i)\n",
    "            \n",
    "            test_id = 'dihard_PLDA_clean_' + vector + '_' + str(models_generation_length) + '_' + str(i)\n",
    "            \n",
    "            a_results = tracking_tester(a_recordings_test_segments,\n",
    "                                        scoring_function = lambda vector, models_container: plda_selector(vector, models_container, b_plda),\n",
    "                                        groundtruth_filepath = a_groundtruth,\n",
    "                                        groundtruth_valid_speakers_ids = ['A', 'B'],\n",
    "                                        vector = vector,\n",
    "                                        models_container_length = models_container_length,\n",
    "                                        models_container_include_overlaps = models_container_include_overlaps,\n",
    "                                        models_generation_length = models_generation_length,\n",
    "                                        models_generation_selection = models_generation_selection,\n",
    "                                        save_dir = 'batch/' + test_id + '_a')\n",
    "        \n",
    "\n",
    "            b_results = tracking_tester(b_recordings_test_segments,\n",
    "                                        scoring_function = lambda vector, models_container: plda_selector(vector, models_container, a_plda),\n",
    "                                        groundtruth_filepath = b_groundtruth,\n",
    "                                        groundtruth_valid_speakers_ids = ['A', 'B'],\n",
    "                                        vector = vector,\n",
    "                                        models_container_length = models_container_length,\n",
    "                                        models_container_include_overlaps = models_container_include_overlaps,\n",
    "                                        models_generation_length = models_generation_length,\n",
    "                                        models_generation_selection = models_generation_selection,\n",
    "                                        save_dir = 'batch/' + test_id + '_b')\n",
    "            \n",
    "            print(a_results)\n",
    "            print(b_results)\n",
    "            output_der = (a_results['der'] + b_results['der']) / 2\n",
    "            output_eer = (a_results['eer'] + b_results['eer']) / 2\n",
    "            output_dcf = (a_results['dcf'] + b_results['dcf']) / 2\n",
    "            results = { 'der': output_der, 'eer': output_eer, 'dcf': output_dcf }\n",
    "            print(results)\n",
    "            \n",
    "            file = open('batch/results.csv', 'a')\n",
    "            file.write(test_id + ', ' + json.dumps(a_results) + ', ' + json.dumps(b_results) + ', ' + json.dumps(results) + '\\n')\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
